{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Categorical and Numerical Features with Text in BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clothing Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Download & Parse\n",
    "Retrieve the .csv file for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\n",
      "To: C:\\workspace\\ksa_nlp_expert\\pytorch_seq2seq\\Womens Clothing E-Commerce Reviews.csv\n",
      "8.48MB [00:01, 8.42MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "print('Downloading dataset...\\n')\n",
    "     \n",
    "# Download the file.\n",
    "gdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H', \n",
    "                'Womens Clothing E-Commerce Reviews.csv', \n",
    "                quiet=False)\n",
    "    \n",
    "print('\\n\\nDONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the dataset csv file into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0          767   33                      NaN   \n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training size: 18,788\n",
      "Validation size: 2,348\n",
      "      Test size: 2,350\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# First, calculate the split sizes. 80% training, 10% validation, 10% test.\n",
    "train_size = int(0.8 * len(data_df))\n",
    "val_size = int(0.1 * len(data_df))\n",
    "test_size = len(data_df) - (train_size + val_size)\n",
    "\n",
    "# Sanity check the sizes.\n",
    "assert((train_size + val_size + test_size) == len(data_df))\n",
    "\n",
    "# Create a list of indeces for all of the samples in the dataset.\n",
    "indeces = np.arange(0, len(data_df))\n",
    "\n",
    "# Shuffle the indeces randomly.\n",
    "random.shuffle(indeces)\n",
    "\n",
    "# Get a list of indeces for each of the splits.\n",
    "train_idx = indeces[0:train_size]\n",
    "val_idx = indeces[train_size:(train_size + val_size)]\n",
    "test_idx = indeces[(train_size + val_size):]\n",
    "\n",
    "# Sanity check\n",
    "assert(len(train_idx) == train_size)\n",
    "assert(len(test_idx) == test_size)\n",
    "\n",
    "# With these lists, we can now select the corresponding dataframe rows using, \n",
    "# e.g., train_df = data_df.iloc[train_idx] \n",
    "\n",
    "print('  Training size: {:,}'.format(train_size))\n",
    "print('Validation size: {:,}'.format(val_size))\n",
    "print('      Test size: {:,}'.format(test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Always Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we always recommend the product...\n",
      "\n",
      "F1: 0.908\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Select the test set samples.\n",
    "test_df = data_df.iloc[test_idx]\n",
    "\n",
    "# Create a list of all 1s to use as our predictions.\n",
    "predictions = [1]*len(test_df)\n",
    "\n",
    "# Calculate the F1 score.\n",
    "f1 = f1_score(y_true=test_df[\"Recommended IND\"], y_pred=predictions)\n",
    "\n",
    "print('If we always recommend the product...')\n",
    "print('\\nF1: %.3f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Threshold on Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend if rating >= 3...\n",
      "\n",
      "F1: 0.955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Predict whether it's recommended based on whether the rating was 3 or higher.\n",
    "predictions = test_df[\"Rating\"] >= 3\n",
    "\n",
    "# Calculate the F1 score.\n",
    "f1 = f1_score(y_true=test_df[\"Recommended IND\"], y_pred=predictions)\n",
    "\n",
    "print('Recommend if rating >= 3...')\n",
    "print('\\nF1: %.3f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0          767   33                      NaN   \n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count  Division Name  Department Name  Class Name  \n",
       "0                        0              2                2           5  \n",
       "1                        4              0                1           3  \n",
       "2                        0              0                1           3  \n",
       "3                        0              1                0          13  \n",
       "4                        6              0                4           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, change the type of the specified columns to \"category\". This will \n",
    "# assign a \"code\" to each unique category value.\n",
    "data_df[\"Clothing ID\"] = data_df[\"Clothing ID\"].astype('category')\n",
    "data_df[\"Division Name\"] = data_df[\"Division Name\"].astype('category')\n",
    "data_df[\"Department Name\"] = data_df[\"Department Name\"].astype('category')\n",
    "data_df[\"Class Name\"] = data_df[\"Class Name\"].astype('category')\n",
    "\n",
    "# Second, replace the strings with their code values.\n",
    "data_df[\"Clothing ID\"] = data_df[\"Clothing ID\"].cat.codes\n",
    "data_df[\"Division Name\"] = data_df[\"Division Name\"].cat.codes\n",
    "data_df[\"Department Name\"] = data_df[\"Department Name\"].cat.codes\n",
    "data_df[\"Class Name\"] = data_df[\"Class Name\"].cat.codes\n",
    "\n",
    "# Display the table--notice how the above columns are all integers now.\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Numerical Features to Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50.0</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47.0</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID   Age                    Title  \\\n",
       "0          767  33.0                      NaN   \n",
       "1         1080  34.0                      NaN   \n",
       "2         1077  60.0  Some major design flaws   \n",
       "3         1049  50.0         My favorite buy!   \n",
       "4          847  47.0         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...     4.0                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...     5.0                1   \n",
       "2  I had such high hopes for this dress and reall...     3.0                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...     5.0                1   \n",
       "4  This shirt is very flattering to all due to th...     5.0                1   \n",
       "\n",
       "   Positive Feedback Count  Division Name  Department Name  Class Name  \n",
       "0                      0.0              2                2           5  \n",
       "1                      4.0              0                1           3  \n",
       "2                      0.0              0                1           3  \n",
       "3                      0.0              1                0          13  \n",
       "4                      6.0              0                4           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the numerical features to floats.\n",
    "data_df[\"Age\"] = data_df[\"Age\"].astype('float')\n",
    "data_df[\"Rating\"] = data_df[\"Rating\"].astype('float')\n",
    "data_df[\"Positive Feedback Count\"] = data_df[\"Positive Feedback Count\"].astype('float')\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15277</th>\n",
       "      <td>875</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22029</th>\n",
       "      <td>909</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15422</th>\n",
       "      <td>854</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>862</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>1078</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID   Age  Rating  Positive Feedback Count  Division Name  \\\n",
       "15277          875  52.0     4.0                      4.0              0   \n",
       "22029          909  68.0     5.0                      3.0              1   \n",
       "15422          854  40.0     3.0                      1.0              1   \n",
       "9191           862  35.0     4.0                      0.0              0   \n",
       "467           1078  61.0     5.0                      1.0              0   \n",
       "\n",
       "       Department Name  Class Name  \n",
       "15277                4           8  \n",
       "22029                4           4  \n",
       "15422                4           8  \n",
       "9191                 4           8  \n",
       "467                  1           3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the labels for each of the splits.\n",
    "y_train = data_df[\"Recommended IND\"].iloc[train_idx]\n",
    "y_val = data_df[\"Recommended IND\"].iloc[val_idx]\n",
    "y_test = data_df[\"Recommended IND\"].iloc[test_idx]\n",
    "\n",
    "# Before selecting the inputs, remove text columns and the labels.\n",
    "data_df = data_df.drop(columns=[\"Title\", \"Review Text\", \"Recommended IND\"])\n",
    "\n",
    "# Select the inputs for the different splits.\n",
    "X_train = data_df.iloc[train_idx]\n",
    "X_val = data_df.iloc[val_idx]\n",
    "X_test = data_df.iloc[test_idx]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using XGBoost on non-text features...\n",
      "\n",
      "F1: 0.956\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create an instance of the classifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Train it on the training set.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to predict the labels for the test set.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score.\n",
    "f1 = f1_score(y_true = y_test,\n",
    "              y_pred = predictions)\n",
    "\n",
    "print('Using XGBoost on non-text features...')\n",
    "print('\\nF1: %.3f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# plot feature importance\n",
    "plot_importance(model)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6], <a list of 7 Text xticklabel objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_gains = model.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "pyplot.bar(feat_gains.keys(), feat_gains.values())\n",
    "pyplot.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 BERT on Review Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BERT with All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 All Features to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features into strings...\n",
      "  DONE.\n",
      "Dataset contains 23,486 samples.\n"
     ]
    }
   ],
   "source": [
    "# This will hold all of the dataset samples, as strings.\n",
    "sen_w_feats = []\n",
    "\n",
    "# The labels for the samples.\n",
    "labels = []\n",
    "\n",
    "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
    "data_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
    "\n",
    "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
    "# replace the NaN values with empty string.\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "\n",
    "print('Combining features into strings...')\n",
    "\n",
    "# For each of the samples...\n",
    "for index, row in data_df.iterrows():\n",
    "\n",
    "    # Piece it together...    \n",
    "    combined = \"\"\n",
    "    \n",
    "    #combined += \"The ID of this item is {:}, \".format(row[\"Clothing ID\"])\n",
    "    combined += \"This item comes from the {:} department and {:} division, \" \\\n",
    "                \"and is classified under {:}. \".format(row[\"Department Name\"], \n",
    "                                                       row[\"Division Name\"], \n",
    "                                                       row[\"Class Name\"])\n",
    "    \n",
    "    combined += \"I am {:} years old. \".format(row[\"Age\"])\n",
    "    \n",
    "    combined += \"I rate this item {:} out of 5 stars. \".format(row[\"Rating\"])\n",
    "    \n",
    "    # Not all samples have titles.\n",
    "    if not row[\"Title\"] == \"\":\n",
    "        combined += row[\"Title\"] + \". \"\n",
    "    \n",
    "    # Finally, append the review the text!\n",
    "    combined += row[\"Review Text\"]\n",
    "    \n",
    "    # Add the combined text to the list.\n",
    "    sen_w_feats.append(combined)\n",
    "\n",
    "    # Also record the sample's label.\n",
    "    labels.append(row[\"Recommended IND\"])\n",
    "\n",
    "print('  DONE.')\n",
    "\n",
    "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This item comes from the Dresses department and General division, and is\n",
      "classified under Dresses. I am 34 years old. I rate this item 5 out of 5 stars.\n",
      "Love this dress!  it's sooo pretty.  i happened to find it in a store, and i'm\n",
      "glad i did bc i never would have ordered it online bc it's petite.  i bought a\n",
      "petite and am 5'8\".  i love the length on me- hits just a little below the knee.\n",
      "would definitely be a true midi on someone who is truly petite.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "print(wrapper.fill(sen_w_feats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 GPU & Transformers Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "desc = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger batch sizes tend to be better, and we can fit this in memory.\n",
    "batch_size = 4\n",
    "\n",
    "# I used a smaller learning rate to combat over-fitting that I was seeing in the\n",
    "# validation loss. I could probably try even smaller.\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Number of training epochs. \n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  204\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sen_w_feats:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a maximum length of 200.\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Tokenize & Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding all reviews in the dataset...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "print('Encoding all reviews in the dataset...')\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sen_w_feats:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Split the samples, and create TensorDatasets for each split. \n",
    "train_dataset = TensorDataset(input_ids[train_idx], attention_masks[train_idx], labels[train_idx])\n",
    "val_dataset = TensorDataset(input_ids[val_idx], attention_masks[val_idx], labels[val_idx])\n",
    "test_dataset = TensorDataset(input_ids[test_idx], attention_masks[test_idx], labels[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples!)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  4,697.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  4,697.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  4,697.    Elapsed: 0:00:14.\n",
      "  Batch   160  of  4,697.    Elapsed: 0:00:19.\n",
      "  Batch   200  of  4,697.    Elapsed: 0:00:23.\n",
      "  Batch   240  of  4,697.    Elapsed: 0:00:28.\n",
      "  Batch   280  of  4,697.    Elapsed: 0:00:33.\n",
      "  Batch   320  of  4,697.    Elapsed: 0:00:37.\n",
      "  Batch   360  of  4,697.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  4,697.    Elapsed: 0:00:46.\n",
      "  Batch   440  of  4,697.    Elapsed: 0:00:51.\n",
      "  Batch   480  of  4,697.    Elapsed: 0:00:55.\n",
      "  Batch   520  of  4,697.    Elapsed: 0:01:00.\n",
      "  Batch   560  of  4,697.    Elapsed: 0:01:04.\n",
      "  Batch   600  of  4,697.    Elapsed: 0:01:09.\n",
      "  Batch   640  of  4,697.    Elapsed: 0:01:14.\n",
      "  Batch   680  of  4,697.    Elapsed: 0:01:18.\n",
      "  Batch   720  of  4,697.    Elapsed: 0:01:23.\n",
      "  Batch   760  of  4,697.    Elapsed: 0:01:27.\n",
      "  Batch   800  of  4,697.    Elapsed: 0:01:32.\n",
      "  Batch   840  of  4,697.    Elapsed: 0:01:36.\n",
      "  Batch   880  of  4,697.    Elapsed: 0:01:41.\n",
      "  Batch   920  of  4,697.    Elapsed: 0:01:45.\n",
      "  Batch   960  of  4,697.    Elapsed: 0:01:50.\n",
      "  Batch 1,000  of  4,697.    Elapsed: 0:01:55.\n",
      "  Batch 1,040  of  4,697.    Elapsed: 0:01:59.\n",
      "  Batch 1,080  of  4,697.    Elapsed: 0:02:04.\n",
      "  Batch 1,120  of  4,697.    Elapsed: 0:02:08.\n",
      "  Batch 1,160  of  4,697.    Elapsed: 0:02:13.\n",
      "  Batch 1,200  of  4,697.    Elapsed: 0:02:17.\n",
      "  Batch 1,240  of  4,697.    Elapsed: 0:02:22.\n",
      "  Batch 1,280  of  4,697.    Elapsed: 0:02:26.\n",
      "  Batch 1,320  of  4,697.    Elapsed: 0:02:31.\n",
      "  Batch 1,360  of  4,697.    Elapsed: 0:02:36.\n",
      "  Batch 1,400  of  4,697.    Elapsed: 0:02:41.\n",
      "  Batch 1,440  of  4,697.    Elapsed: 0:02:45.\n",
      "  Batch 1,480  of  4,697.    Elapsed: 0:02:50.\n",
      "  Batch 1,520  of  4,697.    Elapsed: 0:02:54.\n",
      "  Batch 1,560  of  4,697.    Elapsed: 0:02:59.\n",
      "  Batch 1,600  of  4,697.    Elapsed: 0:03:04.\n",
      "  Batch 1,640  of  4,697.    Elapsed: 0:03:08.\n",
      "  Batch 1,680  of  4,697.    Elapsed: 0:03:13.\n",
      "  Batch 1,720  of  4,697.    Elapsed: 0:03:17.\n",
      "  Batch 1,760  of  4,697.    Elapsed: 0:03:22.\n",
      "  Batch 1,800  of  4,697.    Elapsed: 0:03:27.\n",
      "  Batch 1,840  of  4,697.    Elapsed: 0:03:31.\n",
      "  Batch 1,880  of  4,697.    Elapsed: 0:03:36.\n",
      "  Batch 1,920  of  4,697.    Elapsed: 0:03:40.\n",
      "  Batch 1,960  of  4,697.    Elapsed: 0:03:45.\n",
      "  Batch 2,000  of  4,697.    Elapsed: 0:03:50.\n",
      "  Batch 2,040  of  4,697.    Elapsed: 0:03:54.\n",
      "  Batch 2,080  of  4,697.    Elapsed: 0:03:59.\n",
      "  Batch 2,120  of  4,697.    Elapsed: 0:04:04.\n",
      "  Batch 2,160  of  4,697.    Elapsed: 0:04:08.\n",
      "  Batch 2,200  of  4,697.    Elapsed: 0:04:13.\n",
      "  Batch 2,240  of  4,697.    Elapsed: 0:04:17.\n",
      "  Batch 2,280  of  4,697.    Elapsed: 0:04:22.\n",
      "  Batch 2,320  of  4,697.    Elapsed: 0:04:27.\n",
      "  Batch 2,360  of  4,697.    Elapsed: 0:04:31.\n",
      "  Batch 2,400  of  4,697.    Elapsed: 0:04:36.\n",
      "  Batch 2,440  of  4,697.    Elapsed: 0:04:41.\n",
      "  Batch 2,480  of  4,697.    Elapsed: 0:04:45.\n",
      "  Batch 2,520  of  4,697.    Elapsed: 0:04:50.\n",
      "  Batch 2,560  of  4,697.    Elapsed: 0:04:55.\n",
      "  Batch 2,600  of  4,697.    Elapsed: 0:04:59.\n",
      "  Batch 2,640  of  4,697.    Elapsed: 0:05:04.\n",
      "  Batch 2,680  of  4,697.    Elapsed: 0:05:09.\n",
      "  Batch 2,720  of  4,697.    Elapsed: 0:05:13.\n",
      "  Batch 2,760  of  4,697.    Elapsed: 0:05:18.\n",
      "  Batch 2,800  of  4,697.    Elapsed: 0:05:23.\n",
      "  Batch 2,840  of  4,697.    Elapsed: 0:05:27.\n",
      "  Batch 2,880  of  4,697.    Elapsed: 0:05:32.\n",
      "  Batch 2,920  of  4,697.    Elapsed: 0:05:37.\n",
      "  Batch 2,960  of  4,697.    Elapsed: 0:05:41.\n",
      "  Batch 3,000  of  4,697.    Elapsed: 0:05:46.\n",
      "  Batch 3,040  of  4,697.    Elapsed: 0:05:50.\n",
      "  Batch 3,080  of  4,697.    Elapsed: 0:05:55.\n",
      "  Batch 3,120  of  4,697.    Elapsed: 0:06:00.\n",
      "  Batch 3,160  of  4,697.    Elapsed: 0:06:04.\n",
      "  Batch 3,200  of  4,697.    Elapsed: 0:06:09.\n",
      "  Batch 3,240  of  4,697.    Elapsed: 0:06:14.\n",
      "  Batch 3,280  of  4,697.    Elapsed: 0:06:18.\n",
      "  Batch 3,320  of  4,697.    Elapsed: 0:06:23.\n",
      "  Batch 3,360  of  4,697.    Elapsed: 0:06:27.\n",
      "  Batch 3,400  of  4,697.    Elapsed: 0:06:32.\n",
      "  Batch 3,440  of  4,697.    Elapsed: 0:06:37.\n",
      "  Batch 3,480  of  4,697.    Elapsed: 0:06:41.\n",
      "  Batch 3,520  of  4,697.    Elapsed: 0:06:46.\n",
      "  Batch 3,560  of  4,697.    Elapsed: 0:06:51.\n",
      "  Batch 3,600  of  4,697.    Elapsed: 0:06:55.\n",
      "  Batch 3,640  of  4,697.    Elapsed: 0:07:00.\n",
      "  Batch 3,680  of  4,697.    Elapsed: 0:07:05.\n",
      "  Batch 3,720  of  4,697.    Elapsed: 0:07:09.\n",
      "  Batch 3,760  of  4,697.    Elapsed: 0:07:14.\n",
      "  Batch 3,800  of  4,697.    Elapsed: 0:07:19.\n",
      "  Batch 3,840  of  4,697.    Elapsed: 0:07:23.\n",
      "  Batch 3,880  of  4,697.    Elapsed: 0:07:28.\n",
      "  Batch 3,920  of  4,697.    Elapsed: 0:07:32.\n",
      "  Batch 3,960  of  4,697.    Elapsed: 0:07:37.\n",
      "  Batch 4,000  of  4,697.    Elapsed: 0:07:41.\n",
      "  Batch 4,040  of  4,697.    Elapsed: 0:07:46.\n",
      "  Batch 4,080  of  4,697.    Elapsed: 0:07:50.\n",
      "  Batch 4,120  of  4,697.    Elapsed: 0:07:55.\n",
      "  Batch 4,160  of  4,697.    Elapsed: 0:08:00.\n",
      "  Batch 4,200  of  4,697.    Elapsed: 0:08:04.\n",
      "  Batch 4,240  of  4,697.    Elapsed: 0:08:09.\n",
      "  Batch 4,280  of  4,697.    Elapsed: 0:08:13.\n",
      "  Batch 4,320  of  4,697.    Elapsed: 0:08:18.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap (doesn't seem to work in Colab).\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader to batch our test samples for us. We'll use a sequential\n",
    "# sampler this time--don't need this to be random!\n",
    "prediction_sampler = SequentialSampler(test_dataset)\n",
    "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "   \n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       return_dict=True)\n",
    "\n",
    "    logits = result.logits\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate the F1\n",
    "f1 = f1_score(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('F1 Score: %.3f' % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
