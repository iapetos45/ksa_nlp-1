{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Packed Padded Sequences, Masking, Inference and BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- nlp에서의 개선사항들 추가!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0\n",
      "1.7.1+cu101\n",
      "2.2.4\n",
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torchtext\n",
    "\n",
    "\n",
    "print(torchtext.__version__)\n",
    "print(torch.__version__)\n",
    "print(spacy.__version__)\n",
    "print(np.__version__)\n",
    "\n",
    "# seed 고정\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# spacy 모델 초기화\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# tokenize 함수 정의\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch에서 packed padded sequences를 사용할 때, 실제 sequence가 얼마나 긴지 알려줘야 사용할 수 있어요!!\n",
    "- SRC의 `include_lengths` 인자를 활용하여 이 정보를 얻도록 하죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data field 정의\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            include_lengths = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "# more than 2 freqs만 사용\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "# device 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterator 생성 시, 길이로 정렬해서 input을 넣어줘야 해요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size 및 train/valid/test loader 호출\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch PackedSequence Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 5, 2, 4, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[  1,  16,   7,  11,  13,   2],\n",
    "                       [  1,  16,   6,  15,   8,   0],\n",
    "                       [ 12,   9,   0,   0,   0,   0],\n",
    "                       [  5,  14,   3,  17,   0,   0],\n",
    "                       [ 10,   0,   0,   0,   0,   0]])\n",
    "\n",
    "input_lengths = tensor.ne(0).sum(dim=1)\n",
    "input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 16,  7, 11, 13,  2],\n",
       "        [ 1, 16,  6, 15,  8,  0],\n",
       "        [ 5, 14,  3, 17,  0,  0],\n",
       "        [12,  9,  0,  0,  0,  0],\n",
       "        [10,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lengths, sorted_idx = input_lengths.sort(0, descending=True)\n",
    "tensor = tensor[sorted_idx]\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 5, 4, 2, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lengths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://dl.dropbox.com/s/jl1iymxj6fdtvoe/0705img4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://dl.dropbox.com/s/e1kjq4jsehbixiq/0705img5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "- 달라진 점?\n",
    "    - pack padded sequence로 연산을 더욱 효율적으로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return torch.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,518,917 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 코드에서 달라지는 점!\n",
    "    - packed_padded_sequence를 위해 아까 Field에서 seq_length도 받아왔었다.\n",
    "    - 이 부분을 소스에 반영하여 수정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "            \n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시간이 절반으로 줄어든 것을 확인할 수 있다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 41s\n",
      "\tTrain Loss: 4.688 | Train PPL: 108.616\n",
      "\t Val. Loss: 4.234 |  Val. PPL:  69.015\n",
      "Epoch: 02 | Time: 0m 40s\n",
      "\tTrain Loss: 3.326 | Train PPL:  27.819\n",
      "\t Val. Loss: 3.524 |  Val. PPL:  33.918\n",
      "Epoch: 03 | Time: 0m 40s\n",
      "\tTrain Loss: 2.677 | Train PPL:  14.540\n",
      "\t Val. Loss: 3.324 |  Val. PPL:  27.763\n",
      "Epoch: 04 | Time: 0m 40s\n",
      "\tTrain Loss: 2.264 | Train PPL:   9.618\n",
      "\t Val. Loss: 3.228 |  Val. PPL:  25.223\n",
      "Epoch: 05 | Time: 0m 40s\n",
      "\tTrain Loss: 1.973 | Train PPL:   7.191\n",
      "\t Val. Loss: 3.201 |  Val. PPL:  24.552\n",
      "Epoch: 06 | Time: 0m 40s\n",
      "\tTrain Loss: 1.730 | Train PPL:   5.639\n",
      "\t Val. Loss: 3.200 |  Val. PPL:  24.539\n",
      "Epoch: 07 | Time: 0m 40s\n",
      "\tTrain Loss: 1.547 | Train PPL:   4.697\n",
      "\t Val. Loss: 3.317 |  Val. PPL:  27.588\n",
      "Epoch: 08 | Time: 0m 40s\n",
      "\tTrain Loss: 1.405 | Train PPL:   4.076\n",
      "\t Val. Loss: 3.375 |  Val. PPL:  29.235\n",
      "Epoch: 09 | Time: 0m 40s\n",
      "\tTrain Loss: 1.288 | Train PPL:   3.627\n",
      "\t Val. Loss: 3.400 |  Val. PPL:  29.952\n",
      "Epoch: 10 | Time: 0m 40s\n",
      "\tTrain Loss: 1.186 | Train PPL:   3.273\n",
      "\t Val. Loss: 3.563 |  Val. PPL:  35.257\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성능도 비슷하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.197 | Test PPL:  24.452 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 `translate_sentence` 함수가 어떻게 문장을 생성하는지 뜯어보자!\n",
    "\n",
    "아래와 같은 역할을 수행한다!\n",
    "- 모델의 eval mode를 보장\n",
    "- tokenize되지 않은 input이 들어올 경우 source sentence를 tokenize해줌\n",
    "- source sentence를 encode\n",
    "- 이를 tensor로 바꿔주고 batch dimension 추가\n",
    "- source sentence의 length를 받고 tensor로 변환\n",
    "- source sentence를 encoder에 태워줌\n",
    "- source sentence를 위한 mask준비\n",
    "- `<sos>` token으로 초기화된 출력 문장을 보관할 목록을 만듦\n",
    "- attention values를 위한 tensor 만들기\n",
    "- maximum length를 넘길 때 까지,\n",
    "    - `<sos>` 혹은 last prediction token이 있는 input tensor를 받아온다.\n",
    "    - input, all encoder outputs, hidden state, mask를 decoder에 입력\n",
    "    - attention value 저장\n",
    "    - 다음 token 예측\n",
    "    - 현재 output sentence 확률 예측값 더해주기\n",
    "    - 만일 `<eos>` 토큰이 등장한다면, while 문을 종료\n",
    "- output sentence를 token으로 변환\n",
    "- output sentence와 attention values를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    src_len = torch.LongTensor([len(src_indexes)])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    mask = model.create_mask(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "        attentions[i] = attention\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시각화 함수를 만들어보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    \n",
    "    x_ticks = [''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>']\n",
    "    y_ticks = [''] + translation\n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 idx를 바꾸면서 실험해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['ein', 'schwarzer', 'hund', 'und', 'ein', 'gefleckter', 'hund', 'kämpfen', '.']\n",
      "trg = ['a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting']\n"
     ]
    }
   ],
   "source": [
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['a', 'black', 'dog', 'and', 'a', 'spotted', 'spotted', 'dog', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAJVCAYAAABETiioAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd7gkZZn38e89MwwDEkSCIlHWgBFRUARURFQUJSqiuGZYUdawrhhQxLgqBvQ1ILqirgIGFBCUIKAEE0FJEkQRECRLGmDi/f7xPM0pmjMwzPQ51V3n+7muvuZ0dU33U91dXb96UkVmIkmSpO6a1nYBJEmSNLEMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5okETG97777n7QIERFtl0HqEg840iSIiMjMBfXvj0fEwzJzYdvlkoZRREzPzIyIFSJimbbLI3WBgU+aYBExLes1DCPiQGBP4EntlkoaTr2To4hYAbgS2MfQJy09A580wXo1eRHxZGAD4K3Ab1stlDSEGjV704CdgbOAn2TmvJaLJo28GW0XQJoKIuLrwAuAO4CzM3NerfmzWVeqas3ecsDHgKcC5wAXt1sqqRus4ZMmx7eAVShNuVtBqfmzY7p0Hy8Edgc2Bm6oNX7uJ9JSMvBJAzbe6NvM/D3wHEoN314R8Yy63IOZprT+739mHgX8NzAbeHtEbNnrAytpyRn4pAGqfZB6ffaeEBFb1JGGK2bm+cDzgScC/xMRm4KhT1NXRMzoff8jYpnefpCZ3wc+CNwO7Ns7QZK05MITJ2kwmn3yIuL7wBbAOsAVwAnAZzPzsojYBPg18Dtgn8w8u60yS22pJ0e90bifBR4LXA1clJmfrOu8nlLbdyWwf2b+oa3ySqPOwCcNWER8BXgJ8F6gV6u3J6VG/fmZeV1EPA04Gfgr8ObM/GNb5ZXaEhEPoYzEvR04F3go8EzgEmCHzLwzIl4HvJty4vTpzDy9rfJKo8wmXWmAIuLhwJaUGoujMvMi4DjgMZQavdtqTeA5wLbAI4Cb2yqv1IZGF4b3A7cAu2fmHpn5CuAIyknS8wAy8zvA5ylBcMcWijvl9F8VqLHcricjzGlZpKUwztQqDwP+Dbg0M+dExBOA04Cjgbdn5l0R8dKI+E1m/i4iNsjMOW2UXZpsdVLlbAzCeAJwI6Wmm4h4BbA38N7MPLb2fb09M78dEdcDx7dT8qmj0dT+EGA/Sk64KjMPdPDMaDPwSUuo98NY/94wMy+mNDvdBjwzIs6nhL1fAm+qzVPbArsBf6PU7M1tp/TS5IiIWcDCzJxbB2jMAJahfPdnAnfUKYpeA3wX+EBmHhARywIfjYgzM/PQzPx5fb579jsNVuMqJ8tT5kCcBiTwqIh4HvCGzLRFYkTZpCstgb6w9w3ghIh4KuUg9j3g9cDllObcV2fmHRGxKvBq4OHAdVBG6LZQfGlS1AEZb6bsD9RLpJ0KPL3uP0cDu0bEJ4BDgA9l5qfqf386ZfLlex2nDHsTo3GVk+mUKaQuocwZ+lzg5ZQm9cMjYvX2SqmlYQ2f9CDVqSTm17+PAV4ELAQekpnzI+K7wNOAhwAX1jPmZwNvBF4KPDszb2qp+NKkqSc6jwdeExGrAW8Crqc24QJHAS+m9OX738z8RJ3H8vHAAcCdwGGTX/Kpp/5OzQK+BKwHXJ6ZV8M9v3OvBr4PHBoRr87MG9orrZaEo3SlB6Fv6pUjgU2AXYBDgc9k5tfrY08B3kkZrTsduJUyEvH1mXluG2WXJlOvv179+wjKyc4VwPa1+0NvvWdT9pWdgIOARwJrUWr2NquXIbQZdxLUKaMOo4yWPjQz39F4LICtgf8DzgNel5nXtVJQLRGbdKUHoRH2fgo8izJ1xO+BeZSDVC8Ungf8F7A55WC2K/Aiw56mkOZIzxUpV5l5KPCSiFip90Bmnga8FdgLWJNycvRD4Jk17M0w7E2Mca5ychalJeJy4FURsWvjsaRMJbU75fJ375/EomoArOGTFkNfn72XU6aH+Dxwbm0KOQG4ODPf3qvZsFZCU0nje9+s2VuRUgN+GnADcDClX9gBwDcz89a+57jXqHf3oYnT65pSm9AfAswB5tcBNFsBn6IM2DggM3/S+H9B6bLyJz+b0WINn7QYGmHve5SO5B/OzHMaP3iXAxv11q+d1d8fEc+d9MJK7VgZ7rlUYK9/+AHAbpn518y8LTN3A34F7AO8ue4nRMQj676ybPMJDRQTowbp+TWQH0qpuTsB+EpEzMrMXzFWg/eeiNip93/rrDpn1xPdcefr03Ay8En3o/mDFhHbUyaDPZZyqaemG4E16t/LUWr/3ksdjSt1WUQ8BjglIl4J0BvURBkYeHtdZ9n62G7AKZSrZ+wTEc+njNbdH7h7cks+9dRa1N48e7+nDNA4BrgA2BS4JCIenZmnUH7DEnh3RLy6/7kM5KPFwCfdj0bN3geADYDDM/O3mTmvLu/1gbkAWDki1gb+H2VE23ObndOlDlsTuAvYt1kbBMyiBj5gbu8Eqoa+Eyn99r5PCXovdJqiiRERW0bEu+vd3nu8FzCbMvjiI5m5NyV4r0MJfmTmqcC+lCsC2Vox4gx80gOIiCdTLuD+eUpfF2q/l+Y8ev8EVqD0UXo1ZeqVcya/tNLkq8HgfcA1wCcbnf2XpfQN6zUFLmiEvtdR+ve9hnJyNK/RFKwBiYhNKc21j4R7/WY9ljKd1BV1vV2BD1OucnJYRKwcEcvUmr5XUAbWaIQZ+KQHdgnwWsrM8y+KiLVrx+bm/nMXJfBtRplK4o8tlFOadI0AdyrwaUqA+Ejt+H8F8IjaR2+liFgOWC4ilomIlTPz1Mz8ZS8INpqCNQB1ouu9gd9k5rsjYmZEPL33MDAvyyUgXwUcDuxbr3Iyk9KH7211EM4f7bM3+fpHUS8tA9+I6f8C9ELHoL8YU9V4P2iZOZdyebQPU86Ij4qI5Wvo6x3sfg+8A9jCqVc0VcTYdVdn1lGfp1BGd14JfJkyxcfTgDMpc7ddSDmB+gtlf7qH/cEmRACrA8tGxIaU9/8l9bFDgc0j4v8ol7Tbt3GVk40oJ69zm83sfkaTp/a17I12f1hE7BERuy/Vc9plYnT0TXewCrAdZV6rb2TmnFYL1wF9U69sC6xPGXTxp8y8vM5CvzXwVeAm4DmZOTsilvX911TTmNZjBeBbwLXAfpl5S0RsDbyLcjmuY4DvAKvU2yxKP7JvWqM38SJiI+APlGt8/x14WWZeW+dC/ATwBuCUzHxZHVjzOMoE2HOAbQx5k6sxvdEylFajj1Eux7kL5TN5DHD1kvR3tb/ECGgEvZkRsTLwUcoXYAfKJYp+wdilirQEeiPX6t+HUa7jOZNyEFsmIt6YmedGxMmUvixfBU6OiK0zc3ZrBZdaUH+TemHvTOBq4HeUa0mTmSdHxELK9CubAD/KzCPHeZ4Zhr6JVX+35gEPo0y/0muVuC0ivk4J33tHuXzaypRZBpLSr3JBOBfipKphbytgN2BnSm35HymDn76Umf9Y0ue2SXcE1C/A84EvAn8GNqZcj3I2ZdSoYW8p5dgVNA4CngHsmZnrU5pANgaOiYhNM/Nuyo/mXpTrfR5jc7qmmvqbNA34AuV3aE/gC5l5Z6Obw68o8/BdAxwQEa8Z53kMexOk97sUEVsAX6H8Zu0IfCIi1gXIzAuADwLbAFdRagIPZuySdl7lZBJFxF4R8W1KF6J1Kd0inkEZdHMeZWT7EnfhsoZvyEXEW4HnUC7N9QvgwMz8eES8DHgycGpd757mXi2ZKNf0fCLwrsz8VZ3G4LWUZo8XAD+JiJfWM+ZfUarYLx/1990zeC2hZYGnAGdk5t96C5u1Qpl5Sq3p+xzwYuB7LZV1IEZhX2m89wmQmWdExG9qSL+J0nePiPhQZl6VmbdRTmJPHud5DOSToNaUv5MyN+WZlGPLrzPzlvr4myj9MU+De420flAMfEOqtt9/HngZcDHlwuNn5NiliPai9ME8EZb8CzCVjfPjfRPwI+C4Omptf+ANmfm9iLiK0q/lJxHxmsz8LfVsaxTV0ZI/A/ao/ROH/kCmobMc5Rq5c+Hel0WroW8lYHpm/joi9gT+1F5Rl9wo7SuNQTTLA2+iNN/emJnfA8jMI2rH/+/X9ffNzKv7/m9kMZTb2EWZeUdE/AQ4Arg2M//VqKHdiXJi9apezXo2Lj/4YDhoY4hFmb1+GnB9/QJMqyNDt6dM7vvGzDxpmH+ARkFEvBM4JDNvjYhVM/Om2p/lWuAdvT56EXEhpdP5LZSRh3NGNWjXqRl+Svl+bZ6ZV/o90qIs6iATEccC/wZsmpm3N36jgjJ35SOB/270j13ig1VbRmVfaXT2X4HS52sWJZQvQ+kK9C7g7NpU+3JKbeuhwP6Z2X/lIE2SiFirF7obywLoXRHlC8CWwI796z1Y9uEbQhGxZkTMzMy/ZOYljbTfa7d/DnAzcCk4VH5pRMRalJrUdwLUsLcCsBblQuK9sPcsStB7J/CCzLx7VMNe9Udgd8oo5LMiYt10ni2No/bjWhhl7ry1I2KtKJflgjLZ8qrADyJipUaYewyldWIVylRGwFhf2REz9PtK/Yx6/SrfRRmN+wLKdb+3pYS/wyj9kcnMH1MmiH898O8tFFlARBwAHFj7Wd6jV8MaEU+itOZ9fWnDHljDN3Qi4ouUodjfyzKnVf/jT6Z0rH1HZh482eUbdeONCoyI/wLeThmocUJd9iPK6MK3UYLev1MGaezQaFYfSfVkotcM9wLgQMoVRDbPzGuGsfZC7Wg0861ImV5ldcp0RSdTfqMOj3KFhv8H3Eq5zvRMyknpfErN3/xR7WM8SvtKDeF7U0LeRZn50cZjK1H6e9+ZmZs3lm8FnG5fvclXjzFPpwxsOi4zL+97fCZlrsrtgG0z89qlfU1r+IZIRPyQ0lfvAsrkpP2PT6dMxXIJI9x/rE29H7baLN5zLGUi2F0jYo26bHfgBuAoSv+dHYB3diDsTWscwD5DOXuEMiLstxGxzrDVXqg99buwHKWzeFCmhNqbMg3LoRHxhsz8IWV+yguB51ECx2mMhb3pIxr2Rm1f2Rz4H+CVNGpV60nubZTP7Ym1fzJQRlLXz8j+/JMoIvaj7CevpHQnurzWoC/bWG0hpYb8zEGEPXDQxtCIiA9QZjZ/OXBeZt4ddULfxlnmLMrI3NP7zwa0+CLiQODttf/RB7OMuv0apcnjKOBnmTk3Ip5JuYbk3cC5mXlFe6UejBybfuZgypnj+yjD/bejTMD6h4h4RmZeNUy1F5p8jVq57Sm1dnsD59emw5XraqsBZOaFwE51+d1ZJyIfr0Z9VIzSvlI/qxMjYjvKCey/R8TPM/Ocxvv/D8ZCxL2M6mc0wjYATsvMMwEi4vHAB4BHRsTZwEfrQI6vARfVdZa6ltwaviFQzxAfQwkaf6hhb0PgexFxHHBIRDy89if7APDe+v+c/20xjPM+HUvpj7M18LmI+B/gDMp1QA+OiIfDPf0ofpiZR3ch7PVExJrAc4GvZub/ZbkU3KcoE0rfRKm9eOSQ1V7cS0TMiDISUQMSEZtExGvHeWhNSjPmVTXs7UaZZuW9Wa67unJEbAqQmbc2wl6MepAY1n2lv0auFwQy8xfATpTjyftj7Lq5UEZU30Wj9k+TKyKm16bahwEPjYjtIuL9wNnAY4Ebgf+knFyQmef39qFB1JIb+IZAPTNcAGwTEc+LiP0pUxisTtlBN6JcjHxmZv61N5BgFJtJJtsizorOplwK6uvArykjCX9P2R9uAPatzVhdNYcycm8G3NNPayGlX9YXKe/HKRGx3jDW8NVmj9OB/6h9y7SUIuKhlNrsb48T+u4EHlYHj72EMrLzAzXszaDUdu3SGMgBdOb3aej2ldrUPD8iVoiIr0XEIRHxgYhYrv7eHUWZt3UXymCa/SPi44xdEvIbk1FO3VeW+RHnAvtRJlQ+iDJ4Zv/MfCbl6ho/BTaOMgBnoBy0MSRqjd7XKF+CCymXIjqg1k79lPL7uVObZRxlEfEp4NHAv2fmXRHxQkp/pP0pnZnfStkJocwrtlNmntZGWQdpvMAbZY7HX1L6ZD0/x2bUn1+/b+dRJqD+K2WgyoJhO3hHxFGUGtp3A4dl5u0tF2lkRRkh+ErgcMrB562UKZ++XR9fmzLp+3RgQ0pf1i/Vx55MOWidnJkfmvzSD84o7Sv1hPR3lMmvb6KMvj2VMkL3kiyjqnegHDugXMv4EuAz9TG7a0yiKNPgrEs5tpySmRfWlqQVKTnsL3W9VSn74UWUgZmD/S5lprcWbpQf2HdTflyf2Vj+FGCdxv2VKTvtQZSzzGi77KN2A1YC3k+ZquBSyojbacB7KHPtPbSutxml9uJKYP22yz2A7Z7e+HuZvseeQbk249f7lj8F+BWlL+k6E13GJdimaPz9HUoNzJ69z9Dbg34/n0JpXTig3l+XUvO9EHh9XTaDMor9L5TQ8DDK/G6bUWrGfwPMaHtblvJ9GPp9hTIvW+/vJ9Xjwjr19+1ZlEvYnUrp5z2trvfS+ll+FdiwLvMYMrnfrR9SLlt3bb3NofSHXb5vvcdRWp6uAx43IWVp+82YijfK1RyurwFkbv0yHDjOehtTrmt4Y29n9bZY7+/0cZbNoEx3cwRlEtJj6vt7eH2PZ9X1VgdWaXsbBvAeNIPRAXV7TwKeDaxQl7+F0lz3M0rH/JcA36Zc2me1trdhEds1o/H3LErT7hXAHsBKbZdvVG6UGqug9MX7TV22Yt0nHkk5wVxIqenrvdfvpHQ1uZFy4nR+DRjL1HXus9+Nwm0U9pXe957SvLwKpSb2aGBmY52nMhb6nsRY6Nu1fpb/Bzyx7fd7Kt0oLUhXA8+nzFf5JEpXgPmUmvLevvhpynyPfwE2mrDytP2GTLUb8CFKDdJzKGfJ61IuQH49pWNwb729KFX2l0zkF2AStndSzyZ7wa3+vT3wOkozVDMovA74OaVm44+UGorntv1eDfA9aNYEfL8eBI4AzqXUVPwnJfwGZXLWSynNQrdSTkI2bnsbHui7RKmJ/QWlP+YtwL8oNX0rtl3Opd22SX7dd9Uw8CJK/9UfU2q/12Us9L2prjsDWBt4M/AfNfRM7z3W9nu4hNs/9PsKY12vVqT0Ob6MEjR/CSzb3A5K6LsaOIUS3nvLd6qf5cH01WB6m7DPbSbl5OG7/Z8nZeDPPGCTuuzplHD4qAktU9tvylS7AT8BjmzudMAawCcow+ZfWb8QL6FU+67fdpkf5PZN+lk+JTgfDjy8sewH9Qf7Tkot6ge5d1P5qvX9van+EP5v88d/VG99oWg5Si3Es6k1AZQO2/Mpl71apS5bAXgmpYnuEW1vw2Js46drONmGMgnwU2tQubMGkaGv6WtjPxnvu0KpzTuh7iNncu8To3Up/YrvCX3Dui1Luv2Nv4dyX2EsUE8DjqMEvk/U37eFwKcb6/bCXa+Z/qv1//UC40uBJ7T9vk+VW92/jgeOHGf5WpT5dr/GWA35hB9/nIdvktQRNzMoUxz8PWvnX2BhZl4fEV8FXkOZwf0HwM9jxK47Wcvbu2bmnpT+JecBv8vMqybwpZ9N6cB/bERsSxne/mTKWe0/KaPVPgo8LCK+mJlXZOZNwJcj4hxKn74vjdJ7vSjZ+0Up11/ckVITcVXWCWQzc4+IWEA5wyQivpOZN1D6Yg29OkL3icAJmfnLxkMvj4hDKU1yGRE/zMxbWinkA2hxP7mXzMyIuI4yhcftwKOALYBf1wEMV9YpiwC+ERELsg7k6Huekez8Pwr7So5NfL0NpWXo4Mw8K8oE8ecDH62fyweyDMaYlpnnRcTjgMt7yyKCzDxmsso91fUGAEXEZcCOEfGkzLwA7vneXR0Rt1O6A8yryyf++NN2Cp4KN2qTQP37vymdNreo92c0HjuW0tQ4kmfMje39IaWJ+mpKf59fMIFNH5Sz2FdQzpjOpAyG+UzfOv9FOSP+PH0drOlAEwf3bppajrFJYq+mTKkBsFxjnYMoU/58iBEb8EA5a/51436vf9PalCa3y4B3MOTNu5O9nyyiDGvU78BOlBqk64Gt+tZZF/hy3X9e0vb7NoBtHpl9pf62fYVy7fQraPQvprRSfKB+Lp9YxPaN9LFk1G6UCp2HU1sZKPNXXka58swGjfUeQamtPYBGLeyEl6/tN6jrN+Az9faEev+x9YB1Lvcenbs6pc/efQZvDPuNe49we37djq3qj+mb6/3fA0+fqNeuO80rgXMofSM+WZc3+/T1Qt8BwHptv28T9Fk8sv77UMoI8H8BJzUeb74f36M0jT6s7XIvYlum9d3vnRjtA/wN2L7v8RmUvkt3UGp2hyrItrmfNN+j3ntJX787ygTDvdD33L7HHkU5WR3JvnqLeC+Gal+hXBptu/6DP6V7z7H1t+s1fY+tRgl9c4Evt/2eTuUbpQvAWZRwfiLw5rr8GfX36lLgI5TBTz+q37cJGY27yDK2/SZ1+UY5g7+MMr/bmo3lO1GG899Sd9YPUi7rdQsjMhqXsf4izX4wH6E0nR7CvfsCvXIiDmb0nb1SQt9ujJ2tr16XN0eyvaP+cH6iSwevum0fpszf1Du5WAl4W30vjm6s1zyQDWWfvb5wtH7dlt5I6nUpc1WeAbygsd6adZ97HLBG29tQy9T6ftL/nlJaHD5P6VD+JWDPxjrPYRGhr7HOyO83w7avUE5WDqevZaLx+HMpJzN/A3boe2xVyjV0T6elwT9T/QZ8lzLbxn/UY8xn63Fm3/r46pTLdv6Z0jT/K+Apk17Ott+ort6AT1Kq4J8JPKQuW7bx+GOBAynz8vy17syT/gVYwm1bntJ8umFj2QqUEcULKVMX9NfO9A5mZ9Co2VyKMjQDwT6UGpKVGWve/Qv1aiXjvPdvpYOdl+sB6yxKU0HvQLYyZXDKPyiX7uut2xvdN3QHCO4djr5WfyQvodTMPrYufwJwOaUf00F1G4+kzGG1dtvbUMvY+n7S/57W17+4fk9+TGlGvoEyp2FvnS3r8muAF7b9Pk7QZzM0+wqlBmhbxuYDXYbSPeHpNOZqo/TjO55ystMf+lZufH5Dt093+VY/pz9TZoXoDcB4Rt3Hv9l37FmD0pzbSneT1t+sLt4oZ4s/A/ZrLHsUZULTQ4F9GQuBq1Pa+Vdou9wPYvseT6khWKlv+Rr1QHEHjSkbGo+/oh5s7plOYAlfvxkIfkg56/0sdZQuJfTtWg+2f6TOk7U0rzlst/73trH8dZSpSk4b50B2OXBq22Vf3O2iBLyrKLXgh1GC0knAk+rjG1CmmbioHqTPYYimMGp7PxmnPL15986gBue6/Jv14PS8xrLnUPrDHtP2+zjI71Tf8tb3lfqZ/A74WL2/Yv2OX8jYScHrGuu/gLHQt/14z9f2+z3VbpRuGXcAz6r3H01p1v0+NbAzJFNdtV6ALt1ohDbGBmA8jdJ37E5KU81ZlLb8d9VgMpKdahk7m/wKdQBKvb86ZV67y+uO0H8w24kBTTVDaZa9hjLLfC9A95rQen36LgD+wJA08U3A57AB960len3jQNabXX8lSh+sCxnCK2iMs13rUabKeWVj2Zvr/nMaY6FvWUofuLUZwulYhmE/6Xve44HvN+7vSunz+p56/yGNxzbq/26N8m1Y9xXGpoF5CKVVojfp81Mo00adD7y9sf429dhyI/Dstt/XqXjrO9ZvQwl869X9+mbKtDm9Sbt3pm/asNbK3XYBunSjzHD+ufr3jsBvKUHvAuCDdfkylJqIg9su7xJuY7N2bd36A3UrsGlj+ep12/8+3sFsQOVYvh68vjzOj3hzIMculA78v2YSR0NN0mfxsfqjv1n/dlGuPPFPSl+RXjPoSgzpVUT6vlefBO6mNJNs0rfeGxoH6Me3Xe7F3J5W9hPuffIzgzIR7KnUCd4p00AtBN5X78+iXAXgJeM9zyjfhnVfafxWTae0UpxE7StI6Rd2ff2O/AV4S+P/vZQyYf9IVhiM+o3Gsb7eP4PSX/9flPkcV6zLH06p6TuMIZg1oPU3ris3YFPKGWHvUkQz6g/95r0DE6X6fhVKp+hP1PsjE0DG+3GhVF8fT5nH6xmN5b2D2V8ozRBLddDo//+Uppcrgc/fz/9Zvh7sdgL+re33bwDvf/97sF79zl2wiAPZ/wKzKYNYRmIwUGO7TqthZE/ue23TN1Bqy88DHtN2eccpf2v7Sf93hVIDehLwonp/X8oUI/tQJhX+YOP/PBs4GXh52+/hoLa/7zs1VPsKY2FvJuX6xB8HXlWXHUIZQPKo+t25htK9Ye/F+b55m9DPrXesfwNjNfgvo7Q+3Ea9Wgaln/4hlJOJofj9bb0AXblRRtpec38HIEpH84MpZ21Dd6B6gO1r9q16A/DeetBYh3IJn6PHOZitRumLdC59F4peitd+MXVwC2VU2omMM1UC8Cpgr7bftwG+/82RxqtSR31TmjLPo/Rj26zvvfpUDRM/ZwSu2EKZ1uAwyonQ2pSavL8s4gD9FspAp/XaLndfuVrbTxrP15t6ZWYNDLMpNYxbAf9GCYALuXcNxeMptRTHjXqAGIV9pREUZtXP5lm1fLMowftK7j0IYD/KAL8rgF3bfo+n8o1xjvV1X9utfpY3U1rxzqZ02Xhq22W+p5xtF6ALN8q1Wv8JvLvev0/TIaXP3sn1CzA0HcuXYFuPqD9Gf6Y0Rf2zHnxfTJnm4Tbu3Wy16tL8gPb9KP9f3Yk+VO/vQrmE0Ae592i2VSiDOX5Ko0/SqN0ofdM271vWG6RwLaXpc1YNE+fW5VvV/7cC5exyd2DltrdlcT5nSmf5OcBX6rK1Kf2Xegfo/n1qaLdrsveTxvP0avZWpDQ7HVHfw7soswFsRamNOIHSbP5RStPhmZQBTjOazzMqt1HYVyhXiHlB37InUJqam9N2vZ4y2nz9ej8oI9F/SLms4EgH8lG+Mf6xvldTG5RWpb0pJ3q7MWT9pVsvwCjfGh/07vWHvTdKp/ejuzJjo79eTzlLG9mmReD9lGaFzRgbEfszyhnNiyk1GCdQOhpvPuDX/l49YL2MxnxYlLC3gNJP4lX1s/hxLcPITr1Sfzy+XoJpYYsAACAASURBVH/4X1iXfZFywvBJSt/Fu+q2PrLezqrbfQrl5OLWYf2+jRcoKP1b30AJfV+ry5qh75mjEETa3E8a7+PJlEEhT6f0I3ohpQbrcsZq+j5Iqe07gjIvXS/sjdQ8e6Owr1Ca1n9RvxcvbJR7I0qLzxqNdbeh1MDuUj/Lx1L6IL+isY6hb3K/Yw90rF9lFI43rRdg1G+UhH8x9x75tiJluoVf1B33P+vOPdKX8KLM1XUIY6PK1qk/socy1vSwEaV/1ZU0Ji1dytd9AaWWZOvGsodSzpi3rO/1VZSmq6so0xyMxJyGD7Ddj6vB4HzKDPyfojH/FvAiSk3RUYzNN3ggpdnwJ8AT296GxdjGDfruzwTeyH1D3x8pNTWbTHYZl2CbWtlPGq+/PiXs7N1YFrUcv6WcOD2vLu/vHzmSQWIU9hXGrrJ0PmN9Kjekhk1KuAtKTeRX6rHjEkrz4TmMWBDv2o0HPtbPB97feGzo+ue3XoBRvTHWB+ONNWBsXO9/gNIPZAGlP9J/9P+ojuKtftlPBX5a729AGZH0A8bmGnpz/eF6PLDuAF97F8pceytTpi54HqVv1z/qj+JetXwb1tcf2qa+Jdj2Xp+rC2vg2agu751xPr8eyI6hzjdYl8+c7LIuwbZ9rIaeZ/YtX4YyWGMBZR67aZRO92cwpDWWjbK3tp80yvAwSuD7eN/yoNQwzq2hY/Nemdt+3wa03UO/r1AGYJxU3/9tKLWvV9Fo0q3rLU+ZMuerlIE2vdrXkQzko3x7kMf6of58Wi/AqN8oZ2IXU5oQzqRUzx9Eozaq+aUZ5Vvdxt9RzqB7cw2tXB/bkNKE9KoJeN3HUWp8jq472GxKDcqOlGapuXSgRu9+tv8xlNqLhc33l7HmhK0pzVO/otHvp+1yL8Z2bVIPfMdw39C3Sv28FwLfqMtGooajrf2k8frL1ffut/SNDgTWqoHoCkooHNk+rovY9qHfV2roO7l+Dp+g9An7AOWSXO+gdE3ZgXLSs3bj/43E97+rty4c61svwCjfKE2KC+vtyPqFWIOxZptOXeoGeBJlgsmFlAERvbPO1Siz9Z/HBHVSpfQ7Oply7c/dG8t3ptT2rd/2+zPB7/0GlBquy4FtG8t7B7JtKc2DQ3FZsQexXRvV0PfzcUJfLzhdQ18NyDDf2txP+sowmzLh6+MbyzejBOwtKNN+fKrt92sCtn3o95Ua+k6gtFwspJwUXFo/k0spgfwshrzGaKrcunKs7xVSSyAilqc0z9xCufbiv+ryaZm5sNXCTZCI2Ibyhf895WACpZnoeZSLrZ83ga89A1iQvT0rYg3KGfKTge0y86aJeu1hEBH/Rhl5uDqwT2YeV5dPy8yFEbF8Zt7ZaiGXQERsRBl0cyXw0cz8XUQ8nNKc+1Pg2My8q80yPlht7ieNMmxLGZBxAaXT/7XAayk1SjtTahrPysw9Jrosk20U9pWIeAxlQMl6wLsy8xf1mAJlkMctmZldPp6Miq4c6w18Sykipmfmgsb9yI6/qRGxKfA/lM70Cygdiz+UmRdOYhleTRnMsSOTdAAdBhHxaMqIxDUoUwOc0HKRBiIingJ8izI9yW8otWEbUUbDXd5m2ZbUkOwnT6T0ldyI0ofvz5Q+sTMpk1sfRxlVTNd+t0ZhX6llPAh4BKWMxzePIaMWKCZaRGyZmae39Nojf6w38GmJRMRylIPGAmBeZs6ZxNfejNLcN5cyufIFk/Xaw6AeJL5CabZ7bWae1HKRBiIiNqDMV/lsyoCc92fm+e2Waum0uZ80yjCLUmM0KzOvi4gVgP9HmeLoWZn5l8ku02QZhX2l1kYeRLl27vaZ+fuWizSUIuL5lIn235OZn2u7PKPIwKeRExHTKFMc3JSZN7RdnjZExOOAz1Cagv7WdnkGJSKCMuggMnN22+Xpmoh4AfARyhQtL8vMP7VcpAk3CvtKLePbKGVc8EDrT0UR8VDgv4BDM/Pitsszigx80oiKiJmZObftcmh01Nq9NwDHdblmr98o7Sv9TYcaYxP30jHwSZIkddy0tgsgSZKkiWXgkyRJ6jgDnyRJUscZ+CRJkjrOwDcEImLPtsswKF3Zlq5sB7gtw6or29KV7QC3ZRh1ZTug/W0x8A2Hznyh6c62dGU7wG0ZVl3Zlq5sB7gtw6gr2wEtb4uBT5IkqeOch+8BzJgxM2fOnDWhrzF//jxmzFhmQl8DoFzEYGLNnz+XGTNmTuhrrLDSShP6/AB33Tmb5ZZ/yIS/zs03Xjfhr7Fw4UKmTZv4c7sFC+ZP+Gtk5qR8j/1dlDTCbszM1fsXzmijJKNk5sxZbPi4Z7ZdjIGISTjoT4bnvPjFbRdhYL7/9c+3XYSBueXW69suwsDMmzcSF2V4QNOnT2+7CAMzGScUUkdcMd7CbiQASZIkLZKBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHTYnAFxHPioijI+KaiJgdEX+KiN3bLpckSdJkmNF2ASbJesAZwEHA3cAWwCERsTAzD2u1ZJIkSRNsSgS+zDy893dEBHAqsDawB3CfwBcRewJ7AiyzzKxJKqUkSdLEmBKBLyJWAT4C7ACsBUyvD1093vqZeTBwMMDyy6+Uk1FGSZKkiTIlAh/wbWAz4GPAn4HbgL0oAVCSJKnTOh/4ImIWsB2wd2Ye1Fg+JQasSJIkTYXQsyylCXdOb0FErAhs31qJJEmSJlHna/gy89aIOBPYLyJuAxYC7wNuBVZqtXCSJEmTYCrU8AG8Grgc+C7wReCI+rckSVLndb6GDyAzLwO2Hueh/Se5KJIkSZNuqtTwSZIkTVkGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HEGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HGRmW2XYahFRGfeoFmzVmi7CANx1123t12EgVluuRXbLsLAzJ17V9tFGJiFCxe0XQRJWlJnZ+Ym/Qut4ZMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6btICX0R8OyLOeoB1MiL2HvDrblWf90mDfF5JkqRRYQ2fJElSxxn4JEmSOm7SA19E7BgRF0fE3RFxekQ84X7W3S4iToyI6yPitoj4XUS8cJz1nhIRP4uIWyLijoj4Q0S84H6ed7eImBsRbxnUdkmSJA2ryQ586wGfBz4GvBpYGTg+ImYtYv1HAT8D/h3YBfgN8IuI2KK3QkRsCJwBrAm8BdgJ+CmwznhPGBGvB74L7JmZBy39JkmSJA23GZP8eqsBO2TmbwAi4mzgr8DrgfuEr8z8cu/viJgGnAI8EXgTJeQBfBi4FXh2Zt5Vl5043ovXGr0vAq/NzMMHsD2SJElDb7Jr+K7vhT2AzLwCOBt4xngrR8TaEfGdiLgamA/MA14IPLax2tbADxphb1HeDhwI7PZAYS8i9oyIsx5oVLEkSdIomOwavusXsWzN/oW1Ru9oYEVgP+AyYDbwUWCNxqqrAv9cjNfepT7HLx9oxcw8GDi4liMX47klSZKG1mTX8K2xiGXjBbZHAxsD/5mZ/5uZv87Ms4Dl+ta7iXEC4zh2Bx4C/Cwi+p9DkiSpsyY98EXE5r07EbEu8DTgD+Os2wtlcxrrrwds0bfeScCu9zPwo+cfwPOBxwA/johlHmTZJUmSRtJkB74bgf+LiFdHxE7AsZQm3W+Ps+7FlJD2uTo9y27ACcDVfet9hDLa99SIeGVEbBMR74mIN/Y/YWb+DdiG0mfwe7XZWJIkqdMmO/BcAbwH2B84HLgNeFFm3t2/YmbOAXamDNb4MWUql/8Bft233iXAlpQw+U3KlCwvr691H5l5EWXgx4uAb0REDGC7JEmShlZkOibh/nRp0MasWSu0XYSBuOuu29suwsAst9yKbRdhYObOfaCB8qNj4cIFbRdBkpbU2Zm5Sf9CmzQlSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcTPaLsAoiOhGLp437+62izAQa631mLaLMDBf//kxbRdhYD71tve3XYSBufTSM9suwkBMmza97SIMzLx5c9ougjTSupFkJEmStEgGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HEGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HEGPkmSpI4z8EmSJHXcSAW+iHhSRGREbNV2WSRJkkbFSAU+SZIkPXgGPkmSpI4b6sAXEW+NiKsiYnZE/AxYs+/x5SPiSxFxbUTcHRFnRsQL+9aJiPhYRFwfEbdFxLciYrfaNLz+JG6OJElSK4Y28EXEDsBXgGOAnYHzgW/1rfYN4A3AJ4CdgKuAYyNiy8Y67wQ+ABwEvBy4C/jMhBZekiRpiMxouwD3Y1/guMzcq94/PiJWB94MEBGPB14FvCEzv1OXHQ+cB3wIeFFETAf2AQ7KzP3q85wQEY8C1pm8TZEkSWrPUNbw1aC2MXBU30M/afy9KRDAj3oLMnNhvd+r4VsHeARwdN/z9N/vf/09I+KsiDjrwZdekiRpuAxrDd/qlLJd37e8eX9N4I7MvLNvneuA5SNiWUrYA7ihb53++/eSmQcDBwNERD6IckuSJA2doazhowSy+cAafcub9/8JrBARy/et83DgzsycA1xbl63et07/fUmSpM4aysCXmQuAPwE79D20c+PvM4GkDMQAyojcev/0uugqSujrf57tB1leSZKkYTasTboAnwR+EhFfA34KPBfYtvdgZl4UEYcBX46IlYDLgD2ADYG96joLIuIA4ICIuAE4gxL2nlyfZuFkbYwkSVJbhrKGDyAzfwr8J/Ay4EjKII439a22B/Adyqjco4D1gJdm5umNdb5ACY9vBY4AVqn3AW6bqPJLkiQNi2Gu4SMzvwx8uW9xNB6/kxIK//N+niMpgfBD9zxBxDeBKzPzloEWWJIkaQgNdeAbhIh4EvBK4DeUJtwXUyZrfm+b5ZIkSZosnQ98wGzKvHx7Aw8BrqCEvc+1WShJkqTJ0vnAl5mXA89ruxySJEltGdpBG5IkSRoMA58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkddyMtgswCiKi7SIMxMKFC9suwkDccMNVbRdhYA4/4NC2izAwz9tup7aLMDA33XRN20UYiPnz57ZdhIG57bab2i7CwGR247d4wYIFbRdB48pxl1rDJ0mS1HEGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HEGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HEGPkmSpI4z8EmSJHWcgU+SJKnjpmTgi4iXRkRGxPptl0WSJGmiTcnAJ0mSNJUY+CRJkjpuaANfRDwrIo6OiGsiYnZE/Ckidm88/vraLPvkiDixrnNxROzc9zwREftHxPURcXtEfBdYadI3SJIkqSVDG/iA9YAzgDcDLwOOAA6JiFf1rXcocDSwE/AX4PCIWLvx+NuB/YCDgZcDdwGfmdiiS5IkDY8ZbRdgUTLz8N7fERHAqcDawB7AYY1Vv5CZ36rrnQ1cB7wUOCgipgPvBb6emR+s6x8fEScCa038VkiSJLVvaGv4ImKViPhSRFwBzKu3PYHH9q16Qu+PzLwJuJ4SDAHWAdYEjur7Pz95gNfeMyLOioizlmITJEmShsLQ1vAB3wY2Az4G/Bm4DdgL2KFvvVv67s8FZtW/H1H/vb5vnf7795KZB1OagImIfDCFliRJGjZDGfgiYhawHbB3Zh7UWP5gaySvrf+u0be8/74kSVJnDWuT7rLAdGBOb0FErAhs/yCf5ypK6OuvFdx5nHUlSZI6aShr+DLz1og4E9gvIm4DFgLvA27lQUypkpkLIuIzwGcj4kbgNGAX4PETUGxJkqShNKw1fACvBi4Hvgt8kTIty3eX4HkOBD4JvKU+xwrAPgMqoyRJ0tAbyho+gMy8DNh6nIf2r49/mzKwo///rd93P4EP1VvToUtfSkmSpOE3zDV8kiRJGgADnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6rjIzLbLMNQipuWMGcu0XYyBWLBgfttFGIiIaLsIAzNz5nJtF2FgnrrR89ouwsBstNnmbRdhIP5w6sltF2Fg/vznM9ouwsDMnz+37SIMxMKFC9sugsaRufDszNykf7k1fJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjjPwSZIkdZyBT5IkqeMMfJIkSR1n4JMkSeo4A58kSVLHGfgkSZI6zsAnSZLUcQY+SZKkjpsSgS8inhURR0fENRExOyL+FBG7t10uSZKkyTCj7QJMkvWAM4CDgLuBLYBDImJhZh7WaskkSZIm2JQIfJl5eO/viAjgVGBtYA/AwCdJkjptSgS+iFgF+AiwA7AWML0+dPUi1t8T2HNySidJkjSxpkTgA74NbAZ8DPgzcBuwFyUA3kdmHgwcDBAxLSeniJIkSROj84EvImYB2wF7Z+ZBjeVTYsCKJEnSVAg9y1KacOf0FkTEisD2rZVIkiRpEnW+hi8zb42IM4H9IuI2YCHwPuBWYKVWCydJkjQJpkINH8CrgcuB7wJfBI6of0uSJHVe52v4ADLzMmDrcR7af5KLIkmSNOmmSg2fJEnSlGXgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6rjIzLbLMNQiIiHaLsaA+Flr4kybNr3tIgzMsssu33YRBuLW2//VdhEGZqUVHtp2EQbm7rtnt12EAfGYMqTOzsxN+hdawydJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR13FAFvojYJyK26ls2MyL2j4inDvB19o6IHNTzSZIkDbOhCnzAPsBWfctmAh8GBhb4JEmSppJhC3ySJEkasMUKfBHxxIg4LiJujojZEXFRRLytPvariPhxROwZEX+PiLsi4tiIWKvvOVaLiO9ExE0RcWf9f5s0Hv87sCrw4YjIetsKuL2uckhj+fr1/8yKiM9ExFURMScizo2Il/S97rIR8eWIuKWW/wvAMkv0bkmSJI2gGYu53tHAxcBrgDnA44CVGo8/qy77L2AW8GngSGDTxjpHAo8G/hu4EXgPcEpEbJyZlwE7AacAPwa+Wf/Pn4GtgZOBjwPH1uX/rP/+GHgGpcn3r8CuwNERsUlm/qmu8yngzcC+9fn2AF6xmNstSZI08h4w8EXEasAGwI6ZeX5dfFLfamsAm2fmFfX/XAGcHhHbZuZxEbEtsAWwVWb+uq5zMvB3SvD7j8z8Y0TMB/6Rmb9rvP6Z9c+/9i1/PrBd8zmBEyLisZRw94qIWBV4C/DhzPxc/X/HU4KfJEnSlLA4Tbo3A1cBB0XEKyNijXHWOacX9gAy8wzgekrtG/XfGxrBjMycDRwDbLmEZd8GuBY4IyJm9G6UMNprKn4ypcbxqMbrLmzeH09tnj4rIs5awrJJkiQNjQcMfDUgvZASrr4FXBsRp0XExo3Vrh/nv14PrFn/XhO4bpx1rgMe9qBKPGY14BHAvL7b/sA6dZ1HLKJ845X3Hpl5cGZukpmb3N96kiRJo2Cx+vBl5sXALhGxDPBsSh+9YyNi7brKeLV+azDW1+6fi1jn4ZQaxCVxM3A1sOP9rHNtoyzN1xmvLJIkSZ30oKZlycx5mXky8HlKrd1D60NPi4h1e+tFxBaUUPWHuuj3wBoR8ZzGOstT+uCd3niJuZQmWPqWMc7ykyg1eHdk5ln9t7rO+cDdwA6N153WvC9JktR1izNo4ynAZ4EfAH8DVgHeC5ybmTdHBJQm0mMiYn/GRumek5nHAWTm8RFxBvCDiHgfcBNltO5ywAGNl7sY2C4ijgPuAC7JzNsj4nJg14i4gBLgzgNOBI4HToyITwMXUkYOPxWYlZnvz8ybIuJg4CN1QMiFlFG6KyzZ2yVJkjR6FqeG71pKX7t9gV8AXwUuArZvrPPbuvxA4H+BC7hvU+tOlJB2IPAjIICt65QsPe8BZlOmXzkTeHpd/hZKn71f1uWPzMwEdqb0K3wnJfx9nTJFTLPWcJ+6zn7AYcA1lBpKSZKkKSFKblqKJ4j4FXBjZr58ICUaMuWau9F2MQbEywdr4kybNr3tIgzMsssu33YRBuLW2//VdhEGZqUVHvrAK42Iu++e3XYRBsRjypA6e7xBp15aTZIkqeMMfJIkSR23uJdWW6TM3GoA5ZAkSdIEsYZPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOm9F2AUZDtl0AdVa0XYCBWbhwYdtFGJg5c+5suwgD8amDDm27CAOz5ZY7t12EgbnyyovaLsJAXHrpWW0XQeMaP7NYwydJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR1nIFPkiSp4wx8kiRJHWfgkyRJ6jgDnyRJUscZ+CRJkjrOwCdJktRxBj5JkqSOM/BJkiR13FAFvojYJyK26ls2MyL2j4inDvB19o6IHNTzSZIkDbOhCnzAPsBWfctmAh8GBhb4JEmSppJhC3ySJEkasMUKfBHxxIg4LiJujojZEXFRRLytPvariPhxROwZEX+PiLsi4tiIWKvvOVaLiO9ExE0RcWf9f5s0Hv87sCrw4YjIetsKuL2uckhj+fr1/8yKiM9ExFURMScizo2Il/S97rIR8eWIuKWW/wvAMkv0bkmSJI2gGYu53tHAxcBrgDnA44CVGo8/qy77L2AW8GngSGDTxjpHAo8G/hu4EXgPcEpEbJyZlwE7AacAPwa+Wf/Pn4GtgZOBjwPH1uX/rP/+GHgGpcn3r8CuwNERsUlm/qmu8yngzcC+9fn2AF6xmNstSZI08h4w8EXEasAGwI6ZeX5dfFLfamsAm2fmFfX/XAGcHhHbZuZxEbEtsAWwVWb+uq5zMvB3SvD7j8z8Y0TMB/6Rmb9rvP6Z9c+/9i1/PrBd8zmBEyLisZRw94qIWBV4C/DhzPxc/X/HU4KfJEnSlLA4Tbo3A1cBB0XEKyNijXHWOacX9gAy8wzgekrtG/XfGxrBjMycDRwDbLmEZd8GuBY4IyJm9G6UMNprKn4ypcbxqMbrLmzeH09tnj4rIs5awrJJkiQNjQcMfDUgvZASrr4FXBsRp0XExo3Vrh/nv14PrFn/XhO4bpx1rgMe9qBKPGY14BHAvL7b/sA6dZ1HLKJ845X3Hpl5cGZukpmb3N96kiRJo2Cx+vBl5sXALhGxDPBsSh+9YyNi7brKeLV+azDW1+6fi1jn4ZQaxCVxM3A1sOP9rHNtoyzN1xmvLJIkSZ30oKZlycx5mXky8HlKrd1D60NPi4h1e+tFxBaUUPWHuuj3wBoR8ZzGOstT+uCd3niJuZQmWPqWMc7ykyg1eHdk5ln9t7rO+cDdwA6N153WvC9JktR1izNo4ynAZ4EfAH8DVgHeC5ybmTdHBJQm0mMiYn/GRumek5nHAWTm8RFxBvCDiHgfcBNltO5ywAGNl7sY2C4ijgPuAC7JzNsj4nJg14i4gBLgzgNOBI4HToyITwMXUkYOPxWYlZnvz8ybIuJg4CN1QMiFlFG6KyzZ2yVJkjR6FqeG71pKX7t9gV8AXwUuArZvrPPbuvxA4H+BC7hvU+tOlJB2IPAjIICt65QsPe8BZlOmXzkTeHpd/hZKn71f1uWPzMwEdqb0K3wnJfx9nTJFTLPWcJ+6zn7AYcA1lBpKSZKkKSFKblqKJ4j4FXBjZr58ICUaMl5zVxMr2i6AxjFtWjcuQrT/lw5puwgDc+qRv2y7CANz5ZUXtV2Egbj0UieyGE559niDTrvxqyZJkqRFMvBJkiR13OJeWm2RMnOrAZRDkiRJE8QaPkmSpI4z8EmSJHWc2e0M4gAACw1JREFUgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOs7AJ0mS1HEGPkmSpI4z8EmSJHWcgU+SJKnjDHySJEkdZ+CTJEnqOAOfJElSxxn4JEmSOi4ys+0yDLWI8A2SNJJmzVqh7SIMzDrrbNh2EQbmoCO/3XYRBuJlm27edhEGZt68OW0XYWDmzZtzdmZu0r/cGj5JkqSOM/BJkiR1nIFP+v/t3WusZWddx/Hfn07SOhqKwlBGaOstBiMklAy80QSRgCVeCI2KpIml0pnYQgOmiRdMEW28QbCkjrEMtZlBuSiRQmkTi6X6AgwJE2lsIgGKbS3ay9ALQ29Dp+fvi7UHjzvTaersM3vPM59PsjNnrf3stZ5n5s03a521BwAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAZ3XAVfVb2oqrqqfmrZcwEAOF4cV8EHAMDTJ/gAAAa30sFXVRdV1Z1V9XBVfSrJ1rn3N1fVFVV1d1U9VlVfqKrXzI2pqrqsqu6tqv1VdXVV/crs1vAPHMPlAAAsxcoGX1W9LslfJLkuyTlJbkly9dywDyQ5P8kfJnl9kjuTXF9VP7luzNuTvCPJlUl+McmjSd69oZMHAFghm5Y9gSP43ST/0N0XzrZvqKotSS5Ikqr6sSRvTHJ+d++Z7bshyb8luTTJz1TVSUl+M8mV3f3O2XE+XVU/mOT0JztxVe1IsmMD1gQAcMyt5BW+WaidleSTc299fN3PL0tSST52aEd3r822D13hOz3J85JcO3ec+e3/o7t3dfe27t729GcPALBaVjL4kmzJdPXx3rn967e3Jnmoux+ZG3NPks1VdXKm2EuSfXNj5rcBAIa1qsG3L8nBJM+d279++64k31NVm+fGnJbkke4+kOTu2b4tc2PmtwEAhrWSwdfdTyS5Ocnr5t46Z93PX0jSmR7ESDI9kTvb/uxs152Zom/+OL+wyPkCAKyyVX5o44+SfLyq/jLJNUlekeTsQ29295eq6iNJdlbVM5PcmmR7khcmuXA25omqek+S91TVviSfyxR7L54dZu1YLQYAYFlW8gpfknT3NUkuTvLzST6R6SGON88N255kT6ancj+Z5MwkP9fdn1035vJM8XhRkr9P8r2z7STZv1HzBwBYFat8hS/dvTPJzrndte79RzJF4cVHOEZnCsJLv3OAqquS/Gd3P7jQCQMArKCVDr5FqKoXJXlDkn/JdAv3tZm+rPm3ljkvAIBjZfjgS/Jwpu/le2uS705yR6bYe+8yJwUAcKwMH3zdfVuSVy57HgAAy7KyD20AALAYgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCblj0BOLHVsifAYVSN8e+ytnZw2VNYmG996/5lT2Fhvr7vvmVPYSG2bDl92VNYmAfuv3vZU1iYxx8/cNj9rvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxO8AEADE7wAQAMTvABAAxu07InsIqqakeSHcueBwDAIgi+w+juXUl2JUlV9ZKnAwBwVNzSBQAYnOADABjcCRl8VfWrVXWwqs5c9lwAADbaCRl8mdZ9UpJa9kQAADbaCRl83b27u6u7b1/2XAAANtoJGXwAACcSwQcAMDjBBwAwOMEHADA4wQcAMDjBBwAwOMEHADA4wQcAMDjBBwAwOMEHADA4wQcAMDjBBwAwOMEHADA4wQcAMDjBBwAwOMEHADA4wQcAMDjBBwAwuE3LngA8fbXsCXAYVf5dVs3a2tqyp7AwDz/84LKnsDD7v7F/2VNYiGc/+/nLnsLCPPTQA8uewoZzhQ8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwgg8AYHCCDwBgcIIPAGBwGx58VfXDG32Ow5zzeVW1+VifFwBgFW1I8FXVKVV1blXdlOSr6/Y/o6p+u6puraoDVfWVqjrvMJ9/a1V9dTbm1qr6jbn3X1BVf1dV91bVo1X1taq6bN2Qs5PcVVXvr6qXbcQaAQCOF5sWebCqekmSC5Kcm2RzkmuT/Oy6IX+e5Lwkf5DkX5O8OsnVVXVfd183O8b22bg/S3JDklcmeW9VndzdfzI7zgeTfFeSHUkeTPJDSV647jzXJHlmkvOT7KiqW5L8VZK/7u77F7lmAIBVd9TBV1WnZgq8Nyd5aZKbk/xekr9ZH1dV9SNJLkxyfnfvme2+saq2zsZfV1XPSPKuJLu7+5LZmE/PzvE7VfW+7n4sycuTvLG7PzUb88/r59Td30xyRZIrquqsJL+W5J1J/rSqPpHkqiSf6e4+2vUDAKy6o7qlW1VnJ7kryWVJPpfkrO4+q7uvOMyVtFclWUtyTVVtOvRK8pkkL6mqk5K8IMn3J/nY3Gf/NtMVuxfPtm9O8sdV9aaqOuNIc+zuL3b3xbPjnpfkWZmuHP7HEda1o6r2VtXep/o7AABYdUf7O3wHkjyS5JQkpyZ5VlXVk4x9TpKTknwzyePrXrszXWncOnslyT1znz20/X2zP9+QZG+Sy5PcUVU3V9WrnmKuh+Z4aqZ1P/BkA7t7V3dv6+5tT3FMAICVd1S3dLv7n6rq+Ulen+mW7k1Jbq+q3Un2dPcd64bfn+Rgkp/IdKVv3r353wB97tx7p607Rrr7v5K8aXYL+OWZbgNfW1VndPd9hz40i8+fzvS7fOck+XaSDye5qLu/+P9ZMwDA8eaon9Lt7gPd/dHufnWmhyc+lGR7ktuq6saqOnc29KZMV/hO7e69h3l9O8nXk/x3kl+aO80vJ9mf5Ja5c6919+eT/H6mh0TOTJKqOq2q3pXktiQ3Jjkjya8n2drdYg8AOKEs9Cnd7r49yaWz2Do70xO7u5N8qLu/XFVXJvloVb070y3ZU5L8eJIf7e4Luntt9tn3V9V9Sf4xySsyPezxju5+bPYAxw2ZntT9SpKTk1yS5O4kX5pN5bWZAm9Pkqu6+ztfDQMAcKJZaPAd0t1PJLk+yfVVddq6t96SKdK2Z/pqlv1J/j3TV6Yc+uwHqurkJG9P8rZMV/0u6e7LZ0Mey3Sl721JTs/0O4SfT/Ka7n50NubaTE8JH9yI9QEAHE82JPjW6+571v3cSd43ex3pMzuT7HyS9w5kCsYjfd537QEAzPi/dAEABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAYn+AAABif4AAAGJ/gAAAZX3b3sOay0qtqX5I4NPs1zknxjg89xrIyyllHWkVjLqhplLaOsI7GWVTTKOpJjt5Yzu3vL/E7BtwKqam93b1v2PBZhlLWMso7EWlbVKGsZZR2JtayiUdaRLH8tbukCAAxO8AEADE7wrYZdy57AAo2yllHWkVjLqhplLaOsI7GWVTTKOpIlr8Xv8AEADM4VPgCAwQk+AIDBCT4AgMEJPgCAwQk+AIDB/Q8x9iNdzm0L0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/31695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 31.70\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
