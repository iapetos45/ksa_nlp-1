{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch와 torchtext를 사용하여 sequence에서 sequence로 이동하는 NMT 모델을 구축하자!\n",
    "- many to many를 푸는 모든 문제에 적용할 수 있는 구조에요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- 일반적인 Seq2Seq은 RNN을 사용하여 입력 문장을 단일 벡터로 인코딩하는 Enc-Dec 모델이에요\n",
    "- 위 단일 벡터를 `context vecor`라고 부를게요\n",
    "- context vector는 전체 입력 문장의 추상적인 표현이에요\n",
    "- 이 벡터는 한 번에 한 단어씩 생성하여 대상 문장을 출력하는 방법을 학습하는 Decoder RNN에 의해 Decode되서 타켓 문장을 만들어내요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 이미지는 번역 예시를 보여주고 있어요\n",
    "- guten morgen, 좋은 아침! 이라는 뜻이죠?\n",
    "- 위 문장은 시작 토큰, guten, morgen, 종료 토큰으로 분절(tokenize)되어 Encoder의 input으로 들어가게 되요\n",
    "- 각 time-step별 input $x_t$와 이전 time-step의 hidden state $h_{t-1}$이 각 RNN Cell에서 연산되서 다음 hidden state를 구축하게 되요\n",
    "- 아래와 같이 수식을 쓸 수 있겠군요!\n",
    "\n",
    "$$h_t = EncoderRNN(e(x_t),h_{t-1})$$\n",
    "\n",
    "- RNN의 vanishing gradient, exploding gradient 문제 등으로 해당 Cell은 LSTM 혹은 GRU를 사용해요!\n",
    "- 마지막 time step `<eos>`과 그 이전 시점의 hidden state를 입력으로 context vector `z`를 얻을 수 있어요!\n",
    "\n",
    "$$z = EncoderRNN(e(x_T),h_{T-1})$$\n",
    "\n",
    "- 이제 decoder에 어떤 값이 들어가는지 확인해볼까요?\n",
    "- good morning을 출력하기 위해 문장 시작 토큰 `<sos>`과 context vector `z`를 Decoder RNN에 넣어줄 거에요\n",
    "\n",
    "$$s_t = DecoderRNN(d(y_t),s_{t-1})$$\n",
    "\n",
    "- `e`와 `d`는 보통 같은 Cell로 구현이 되서 같은 연산을 실시해요! (e.g., LSTM, GRU)\n",
    "- 여기선 인코더와 디코더의 파라미터를 사용한다는 것을 강조하기 위해 표기를 구분지었어요\n",
    "- 이게 굉장히 중요할 수 있는데요, 보통 huggingface에선 `lm_head`로 구현되어 있어요\n",
    "- decoder의 hidden state로 어떻게 단어 토큰을 생성해낼까요?\n",
    "- decoder output의 shape을 잘 생각해보면, `(batch_size, 1, hidden_dim)`일거에요\n",
    "    - 각 time-step별!\n",
    "- 이를 squeeze해주고 `hidden_dim` -> `vocab_size`로 보내주는 linear transformation을 취해주고 softmax를 통해 확률로 만들어주면?\n",
    "- 해당 step에서 어떤 token이 나올지에 대한 확률 비스무리한 것이 나오게 될거에요!\n",
    "    - softmax는 calibration을 하지 않는 이상, 확률이 아니에요 ㅎㅎ\n",
    "- 학습 땐 정답을 decoder input에 넣어주는 teacher forcing을 할거고\n",
    "- 보통 추론 때는 위 확률 분포에서 token을 추출하여 다음 decoder input에 넣어주는 input feeding을 실시해요\n",
    "- 추론을 그렇다면 언제까지 하는가? max output length를 설정하거나 `<eos>` token이 나올 때까지 계속 뽑는답니다.\n",
    "- 확률 분포에서 argmax로 token을 뽑을 수도 있고 greedy decoding이나 beam search, sampling 기법의 top-k, top-p, 혹은 Meena에서 사용한 softmax calibration 기법을 사용할 수도 있어요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리의 예측 문장은 $\\hat{Y}=\\{\\hat{y_1},\\hat{y_2},\\cdots,\\hat{y_T}\\}$이 될거고 타켓 문장은 $Y=\\{y_1,y_2,\\cdots,y_T\\}$이 될 거에요!\n",
    "- 학습 땐 둘의 길이가 같겠죠? (왜냐면 teacher forcing!)\n",
    "- cross entropy loss로 정답을 구할거구요\n",
    "- 추론 땐 정답이 없어요! BLEU score 등을 구할 때 원하는 candidate set과 우리가 추론한 정답의 길이는 다를 수 있다는 것!\n",
    "- 예를 들어,\n",
    "    - x = \"아버지가 방에 들어가신다\"\n",
    "    - golden_y = \"아버지가 방에 들어가셨다는 사실을 알고있니?\"\n",
    "    - inference_y = \"아버지가 밤에 도대체 어딜 들어가셨다는거야?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "- 자, 데이터를 준비해보자구요 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu101'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.5'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seed를 고정해줄거에요.\n",
    "- 아래 코드는 기억해두시는게 좋아요! 아님 git gist에 적어두시거나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음은 tokenizer를 만들겁니다.\n",
    "- tokenizer는 말 그대로 분절하는 것을 의미해요!\n",
    "- 아래와 같은 식으로 말이죠.\n",
    "    - good morning! --> [\"good\", \"morning\", \"!\"]\n",
    "- 즉, token들의 sequence죠?\n",
    "- 여기서 `!`와 같이 중요하지 않을 수 있는 토큰들은 stop_words 처리해주는 것이 model performance 상으로 괜찮을 수 있어요.\n",
    "- spacy는 위와 같은 작업을 해주지만, 현업에 가면 직접 사전을 구축해서 넣어주는 일이 필요할 수 있어요!\n",
    "- 그러니까 언젠가는 사전 관련 내부 코드 등을 뜯어봐야 하는 날이 올 수도 있다는 얘기죠 ㅎㅎ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- 아래의 명령어로 코드를 받으세요!\n",
    "```\n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 tokenizer function을 만들어줄겁니다!\n",
    "- 원래 source의 예시에는 source sentence를 뒤집어서 학습하는데\n",
    "    - 논문의 구현에서, 이렇게 하면 최적화가 더 손쉽답니다\n",
    "    - 그냥 backward pass로만 학습...\n",
    "- 저는 그냥 진행했어요! 그래도 성능이 더 좋습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torchtext`의 `Field` 객체는 data를 어떻게 처리할 지 다룬다고 하네요\n",
    "- 아래와 같이 argument를 넣어주면 다음과 같이 문장이 전처리됩니다.\n",
    "    - Good Morning\n",
    "    - good morning\n",
    "    - [\"good\", \"morning\"]\n",
    "    - [\"`<sos>`\", \"good\", \"morning\", |\"`<eos>`\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래의 코드로 train, valid, test 데이터셋을 다운로드 받을 겁니다!\n",
    "- 데이터 셋은 Multi30K라는 기계번역 데이터셋을 활용할 거구요\n",
    "- 30K의 parallel En<->Ge, Fr로 되어있어요!\n",
    "- 문장 당 단어는 최대 12 단어이구요\n",
    "- `exts`는 source/target 언어를 지정해주는 인자에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터를 올바르게 불러왔는지 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원래 github source와는 다르게 반전안할거에요!\n",
    "- 저는 그게 더 성능이 좋더라구요\n",
    "- 휴리스틱은 음... 글쎄요 해보고 믿읍시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 vocab을 설정할건데요, 이것도 nlp에서 중요한 작업입니다\n",
    "- 별로 나오지 않는 단어를 설정하면 lm_head의 차원만 커지니 비효율적이에요!\n",
    "- 전략적으로 다가가야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch device 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch iterator 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- 2 layer LSTM\n",
    "- input sentence $X$\n",
    "- hidden state at layer i $H^i=\\{h_1^i,h_2^i,\\cdots,h_T^i\\}$\n",
    "- first layer hidden state each time steps\n",
    "\n",
    "$$h_t^1=\\text{EncoderRNN}^1(e(x_t),h_{t-1}^1)$$\n",
    "\n",
    "- second layer hidden state each time steps\n",
    "\n",
    "$$h_t^2=\\text{EncoderRNN}^2(h_{t}^1,h_{t-1}^2)$$\n",
    "\n",
    "- context vector each layers\n",
    "\n",
    "$$z^1=h_T^1$$\n",
    "$$z^2=h_T^2$$\n",
    "\n",
    "- LSTM은 hidden state에 cell state도 있는거 기억하시죠?\n",
    "- 어려운 수식 말고 한 번 직접 짜봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 헷갈리지만 중요하고 유용한 사실들!\n",
    "- 본 source code에서 input의 모양은 `(sequence_length, batch_size, hidden_dim)`이에요\n",
    "- 하지만 보통은 그렇게 넣어주지 않죠! `(batch_size, sequence_length, hidden_dim)`으로 넣어준답니다.\n",
    "- 어떻게 설정할까요? 그리고 cell state...?\n",
    "- input/output 어떻게 나오고 들어오는지 한 번 뜯어봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM default values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 5, 10) # (batch_size, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 5, batch_first=False, bidirectional=False, num_layers=1)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden, cell) = rnn(x.permute(1, 0, 2)) # permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # (seq_len, batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 5]), torch.Size([1, 3, 5]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size(), cell.size() # (bidirect * num_layers, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM batch_first=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 5, batch_first=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 5, batch_first=True, bidirectional=False, num_layers=1)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden, cell) = rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # (batch_size, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 5]), torch.Size([1, 3, 5]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size(), cell.size() # (bidirect * num_layers, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM batch_first=True, num_layers=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 5, num_layers=2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 5, batch_first=True, bidirectional=False, num_layers=2)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden, cell) = rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # (batch_size, seq_len, hidden_dim)\n",
    "              # layer만 높인거라 size에 변화 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size(), cell.size() # (bidirect * num_layers, seq_len, hidden_dim)\n",
    "                           # layer별 결과도 가져오기 때문에 size up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM batch_first=True, num_layers=2, bidirectional=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 5, num_layers=2, bidirectional=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 5, batch_first=True, bidirectional=True, num_layers=2)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden, cell) = rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 10])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # (batch_size, seq_len, hidden_dim * 2)\n",
    "              # forward and backward pass 둘을 concat해주기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5, 5]), torch.Size([4, 5, 5]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size(), cell.size() # (bidirect * num_layers, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dropout은 n_layer>=2부터**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 5, num_layers=2, batch_first=True, dropout=0.2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LSTM(10, 5, batch_first=True, num_layers=2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinma\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(10, 5, batch_first=True, dropout=0.2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LSTM(10, 5, batch_first=True, num_layers=1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lm_head 추가\n",
    "- 단일 토큰별로 처리하도록 forward가 구성되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        # lm_head\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "- 입력/소스 문장 받기\n",
    "- 인코더를 활용하여 context vector 생성\n",
    "- 디코더를 활용하여 예측 문장 / 목료 문장 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the seq2seq model\n",
    "- 모델 초기화\n",
    "- 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파라미터를 -0.08 ~ -0,08의 uniform 분포로 초기화시키자!\n",
    "- `nn.init.uniform_`은 in-place 인자임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 14M의 parameter를 가지고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,899,013 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam optimizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross Entropy Loss 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CELoss는 어떻게 동작할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(10, 3, 30000) # (seq_len, bsz, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0828,  0.4125,  0.3340,  ...,  0.7334,  0.3160,  0.6537],\n",
       "         [-2.4486,  0.1952, -0.2943,  ..., -1.9048,  1.5262, -1.1121],\n",
       "         [-1.6414, -1.0252, -1.4527,  ...,  0.6101, -0.2816, -0.0311]],\n",
       "\n",
       "        [[ 0.9988,  0.0054,  1.3491,  ...,  0.8649, -1.0560,  1.4901],\n",
       "         [ 0.8748, -0.5448, -1.5865,  ...,  0.0784, -1.8257, -0.0904],\n",
       "         [ 0.4253, -0.7102, -1.3635,  ..., -2.2506,  0.5756,  0.4709]],\n",
       "\n",
       "        [[-0.0050, -0.1499,  0.2189,  ...,  0.5449, -0.9731, -0.0635],\n",
       "         [ 1.2211, -1.7810, -0.0653,  ..., -0.0360, -1.1089, -0.4934],\n",
       "         [ 0.5145, -0.7971,  0.7535,  ...,  0.1521, -0.0868,  0.1423]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.9344, -1.9056, -1.3603,  ..., -0.3813, -0.7312,  0.0287],\n",
       "         [-0.7076,  1.0310,  1.7773,  ..., -0.6313, -0.7449, -0.9435],\n",
       "         [-0.7282,  0.0181,  1.0175,  ..., -0.0327, -0.8533, -0.4392]],\n",
       "\n",
       "        [[ 0.2228,  0.9212, -0.5597,  ...,  0.2300,  2.3570, -1.0053],\n",
       "         [-0.3954, -0.9313, -1.0421,  ..., -0.6412,  0.4522, -0.1086],\n",
       "         [-0.1530,  0.7645,  0.1461,  ..., -0.5301, -1.4168, -0.4894]],\n",
       "\n",
       "        [[ 0.9571,  0.9169,  0.8537,  ..., -0.8308, -0.6171, -0.4245],\n",
       "         [ 0.8324, -1.2472, -0.9454,  ...,  1.2586, -1.1294,  0.6135],\n",
       "         [ 0.5690, -0.2706, -0.9145,  ..., -1.3785, -1.4426,  1.2661]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19241,  2759, 10354],\n",
       "        [14284,  4891, 17877],\n",
       "        [17212, 21004, 19434],\n",
       "        [20941,   732, 26679],\n",
       "        [16302,    87, 17223],\n",
       "        [29049,  4606, 14762],\n",
       "        [ 7922, 21280, 12773],\n",
       "        [    0, 16778, 25911],\n",
       "        [    0, 26360,     0],\n",
       "        [    0, 24974,     0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.LongTensor(10, 3).random_(1, 29999)\n",
    "y[-3:, 0] = 0\n",
    "y[-2:, 2] = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8252)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(y_pred.view(-1, 30000), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8252)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.NLLLoss(ignore_index=0)(\n",
    "    torch.log_softmax(y_pred.view(-1, 30000), dim=-1), \n",
    "    y.view(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRG_PAD_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 iteration에서 아래의 과정을 수행합니다!\n",
    "\n",
    "1. source / target 문장을 batch로 받습니다\n",
    "2. model의 gradient를 0으로 조정해줘요!\n",
    "3. 모델에 src/tgt를 넣어줘서 예측값을 받습니다\n",
    "4. Cross Entropy로 loss를 계산합니다.\n",
    "5. `loss.backward()` 메서드로 gradient를 계산합니다.\n",
    "6. exploding gradient를 방지하기 위해 gradient를 clip해줍니다\n",
    "7. optimizer class에 정의된 최적화 기법으로 파라미터를 update해줍니다.\n",
    "8. total loss를 더해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.eval()`을 해주면 model의 dropout이 동작하지 않아요!\n",
    "- `torch.no_grad`를 통해 gradient를 계산하지 않도록 해줍니다.\n",
    "    - context manager\n",
    "- 추론에선 teacher forcing을 사용하면 안되요! 정답이 없기 때문이죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습에 소요된 시간을 체크해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best 경신 때마다 그 때의 parameter를 저장할거에요!\n",
    "- Cross Entropy에 exponential을 취해주면 PPL이 나와요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-8/03-perpexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 35s\n",
      "\tTrain Loss: 4.827 | Train PPL: 124.780\n",
      "\t Val. Loss: 4.798 |  Val. PPL: 121.305\n",
      "Epoch: 02 | Time: 0m 35s\n",
      "\tTrain Loss: 4.147 | Train PPL:  63.252\n",
      "\t Val. Loss: 4.362 |  Val. PPL:  78.403\n",
      "Epoch: 03 | Time: 0m 35s\n",
      "\tTrain Loss: 3.809 | Train PPL:  45.086\n",
      "\t Val. Loss: 4.238 |  Val. PPL:  69.298\n",
      "Epoch: 04 | Time: 0m 35s\n",
      "\tTrain Loss: 3.607 | Train PPL:  36.860\n",
      "\t Val. Loss: 4.024 |  Val. PPL:  55.916\n",
      "Epoch: 05 | Time: 0m 35s\n",
      "\tTrain Loss: 3.414 | Train PPL:  30.384\n",
      "\t Val. Loss: 3.953 |  Val. PPL:  52.116\n",
      "Epoch: 06 | Time: 0m 35s\n",
      "\tTrain Loss: 3.247 | Train PPL:  25.725\n",
      "\t Val. Loss: 3.880 |  Val. PPL:  48.437\n",
      "Epoch: 07 | Time: 0m 35s\n",
      "\tTrain Loss: 3.093 | Train PPL:  22.041\n",
      "\t Val. Loss: 3.790 |  Val. PPL:  44.264\n",
      "Epoch: 08 | Time: 0m 36s\n",
      "\tTrain Loss: 2.938 | Train PPL:  18.882\n",
      "\t Val. Loss: 3.725 |  Val. PPL:  41.452\n",
      "Epoch: 09 | Time: 0m 36s\n",
      "\tTrain Loss: 2.817 | Train PPL:  16.726\n",
      "\t Val. Loss: 3.761 |  Val. PPL:  42.985\n",
      "Epoch: 10 | Time: 0m 35s\n",
      "\tTrain Loss: 2.698 | Train PPL:  14.844\n",
      "\t Val. Loss: 3.703 |  Val. PPL:  40.564\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.709 | Test PPL:  40.803 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
