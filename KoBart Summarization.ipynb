{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fbff2080bc386d28\n",
      "Reusing dataset json (C:\\Users\\jinma\\.cache\\huggingface\\datasets\\json\\default-fbff2080bc386d28\\0.0.0\\83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"summarization/train.jsonl\"}\n",
    "split = {\"train\":\"train[:80%]\", \"valid\":\"train[80%:]\"}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['media', 'id', 'article_original', 'abstractive', 'extractive'],\n",
       "        num_rows: 34242\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['media', 'id', 'article_original', 'abstractive', 'extractive'],\n",
       "        num_rows: 8561\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media</th>\n",
       "      <th>id</th>\n",
       "      <th>article_original</th>\n",
       "      <th>abstractive</th>\n",
       "      <th>extractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>대전일보</td>\n",
       "      <td>332585287</td>\n",
       "      <td>[[보령] 보령시가 중소기업 경쟁력을 강화하기 위해 추진해 온 산·학·관 협력기술개발 지원사업이 큰 결실을 맺었다., 시는 29일 오전 보령엔지니어링에서 김인영 대표와 미얀마 조달청 구매담당 과장 등 관계자가 참석한 가운데 미얀마와 자동포구 청소기에 대한 200억 원대 수출계약을 체결했다고 밝혔다., 이날 수출계약은 지난해 8월 미얀마 조달청에서 추진한 샘플 입찰전에서 미국과 중국, 프랑스 등 11개의 유명한 업체들과 경쟁에서 따냈다., 우수한 제품 성능을 국제적으로 인정받게 된 것이다., 이번 협약 이후 105mm와 155mm 자동포구 청소기 각 10세트를 오는 4월에는 기갑부대 2종 각 10세트를 납품할 예정이다., 또한 지난해 7월에는 방글라데시와 샘플계약으로 오는 2020년까지 200억 원 규모의 수출 성과를 이뤘고, 올해 5월 이후에는 터키와 500억 원 대 수출계약을 체결할 것으로 예상되고 있으며 파키스탄과 사우디아라비아, 말레이시아, 태국, 이집트, 베트남 등 6개국과도 수출 교섭중에 있다., 특히 이번 수출계약은 그동안 보령시가 자금사정 등으로 어려움을 겪고 있는 지역 중소 제조업체에 아주자동차대학의 우수한 전문기술을 활용해 기업의 제품개발, 공정개선 등 애로사항 해결을 통한 경쟁력 강화를 위해 추진해 온 산., 학.관 협력 기술개발이 밑바탕이 돼 더욱 주목받고 있다., 이번에 보령엔지니어링이 대표 제품으로 내놓은 자동포구 청소기는 지난 2017년 보령시 산·학·관 협력 기술개발 사업으로 개발됐으며 청소 속도, 회전 속도 향상으로 청소시간을 획기적으로 단축하고, 최적화된 설계를 통한 원가절감으로 가격경쟁력 또한 우수해 특허 등록도 완료했다., 아울러 지속되는 수출 협약에 따라 시와 보령엔지니어링은 현재 15명의 지역 고용 채용 규모를 오는 2020년까지 50명으로 3배 이상 늘리는 등 기업의 성장과 함께 지역과 상생하는 사회적 가치를 실현해 나갈 것으로 기대를 모으고 있다., 김동일 시장은 \"전국 제일의 기업하기 좋은 도시로 만들어 나가기 위해 기업 안정 자금은 물론 기술개발을 지원해 온 것이 큰 결실을 맺게 됐다\"며 \"시는 앞으로도 중소기업의 경쟁력 강화와 해외수출 확대 지원으로 지역경제를 활성화하기 위해 최선을 다하겠다\"고 말했다., 보령엔지니어링은 육·해·공군의 전투장비 및 무기체계 핵심부품을 생산하는 회사로 국가산업 발전 및 전력증강 사업에 기여하는데 역점을 두고 회사를 운영해오고 있으며, 지난해에는 연매출 30억 원을 올렸다.]</td>\n",
       "      <td>29일 보령 엔지니어링이 미얀마와 자동포구 청소기에 대한 200억 원대 수출계약 체결은 보령시의 산·학·관 협력 기술개발 지원사업이 큰 결실을 맺은 것으로써 이 청소기는 지난 2017년 보령시 산·학·관 협력 기술개발 사업이 바탕을 이루어 개발된 제품이다.</td>\n",
       "      <td>[0, 1, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>대구일보</td>\n",
       "      <td>346689275</td>\n",
       "      <td>[대구시는 29일 시청 별관에서 빅데이터 통합 플랫폼 구축 중간보고회를 개최한다., 이는 지난해 5월 행정안전부의 지역 빅데이터 허브 사업 공모에서 대구시가 선정돼 추진하는 것이다., 빅데이터 통합플랫폼은 여기저기 흩어져 있던 데이터를 체계적으로 수집·저장하고, 수집된 데이터를 검색·활용할 수 있는 데이터셋과 데이터 맵을 구축하는 사업이다., 국비 5억 원 등 총사업비 10억 원을 투입한다., 대구시와 구·군청이 보유한 공공데이터 775건을 개방하고 1만4천여 건의 통계데이터를 제공한다., 지역개발, 문화관광, 과학기술 등 12개 카테고리로 데이터를 분류하고 주요 통계데이터나 인기·최신 데이터를 앞쪽에 둬 이용 편의를 높인다., 전기차 충전 인프라 입지분석, 우리 동네 사업 분석 등 그동안 시에서 수행한 빅데이터 분석 사례와 관련 데이터도 다운 받을 수 있다., 데이터는 파일이나 응용프로그램 인터페이스(API) 형태로 제공해 누구나 쉽게 이용할 수 있다., 대구시는 빅데이터 통합플랫폼 구축이 완료되면 빅데이터 활용센터와 연계해 행정 내부는 물론 학생, 창업자 등 시민 누구나 대구시 공공데이터를 활용, 분석할 수 있는 환경도 제공한다., 배춘식 대구시 데이터통계담당관은 “올해를 대구시 데이터 기반 행정의 원년으로 삼고 데이터 행정 추진을 위한 조직 신설, 로드맵 마련, 전문 인력 충원, 직원교육 실시 등을 추진하고 있다”고 설명했다.]</td>\n",
       "      <td>대구시는 빅데이터 플랫폼을 구축하여 데이터를 인터페이스 형태로 제공해 시민 누구나 대구시 공공데이터를 활용, 분석할 수 있는 환경을 제공한다.</td>\n",
       "      <td>[7, 8, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"], num_examples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hyunwoongko/kobart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [271, 16997, 20858, 243, 19133, 15868, 1700, 14889, 300, 18482, 16884, 15265, 22038, 232], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[271, 16997, 20858, 243, 19133, 15868, 1700, 14889, 300, 18482, 16884, 15265, 22038, 232], [283, 303, 15868, 1700, 15868, 1700, 15195, 310, 16160, 14879, 18482, 16884, 15265, 22038, 245]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[271, 16997, 20858, 243, 19133, 15868, 1700, 14889, 300, 18482, 16884, 15265, 22038, 232], [283, 303, 15868, 1700, 15868, 1700, 15195, 310, 16160, 14879, 18482, 16884, 15265, 22038, 245]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['media', 'id', 'article_original', 'abstractive', 'extractive'],\n",
       "    num_rows: 34242\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + \"\".join(doc) for doc in examples[\"article_original\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"abstractive\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c26741d9aa54821b0db5405c31aa2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1898efd670044c297ab574b524c6cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstractive', 'article_original', 'attention_mask', 'extractive', 'id', 'input_ids', 'labels', 'media'],\n",
       "        num_rows: 34242\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['abstractive', 'article_original', 'attention_mask', 'extractive', 'id', 'input_ids', 'labels', 'media'],\n",
       "        num_rows: 8561\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"hyunwoongko/kobart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article_original, media, extractive, abstractive.\n",
      "***** Running training *****\n",
      "  Num examples = 34242\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 51363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51363' max='51363' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51363/51363 2:30:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.390800</td>\n",
       "      <td>1.380210</td>\n",
       "      <td>29.550200</td>\n",
       "      <td>9.444000</td>\n",
       "      <td>29.019400</td>\n",
       "      <td>28.992100</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.166000</td>\n",
       "      <td>1.361020</td>\n",
       "      <td>28.739800</td>\n",
       "      <td>9.239700</td>\n",
       "      <td>28.227400</td>\n",
       "      <td>28.188700</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>1.379811</td>\n",
       "      <td>29.919400</td>\n",
       "      <td>9.617100</td>\n",
       "      <td>29.352800</td>\n",
       "      <td>29.339000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-summarization\\checkpoint-500\n",
      "Configuration saved in test-summarization\\checkpoint-500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to test-summarization\\checkpoint-1000\n",
      "Configuration saved in test-summarization\\checkpoint-1000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to test-summarization\\checkpoint-1500\n",
      "Configuration saved in test-summarization\\checkpoint-1500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to test-summarization\\checkpoint-2000\n",
      "Configuration saved in test-summarization\\checkpoint-2000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-2500\n",
      "Configuration saved in test-summarization\\checkpoint-2500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-3000\n",
      "Configuration saved in test-summarization\\checkpoint-3000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-3500\n",
      "Configuration saved in test-summarization\\checkpoint-3500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-4000\n",
      "Configuration saved in test-summarization\\checkpoint-4000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-4500\n",
      "Configuration saved in test-summarization\\checkpoint-4500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-5000\n",
      "Configuration saved in test-summarization\\checkpoint-5000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-5500\n",
      "Configuration saved in test-summarization\\checkpoint-5500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-6000\n",
      "Configuration saved in test-summarization\\checkpoint-6000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-6500\n",
      "Configuration saved in test-summarization\\checkpoint-6500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-7000\n",
      "Configuration saved in test-summarization\\checkpoint-7000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-7500\n",
      "Configuration saved in test-summarization\\checkpoint-7500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-7500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-8000\n",
      "Configuration saved in test-summarization\\checkpoint-8000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-8500\n",
      "Configuration saved in test-summarization\\checkpoint-8500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-8500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-9000\n",
      "Configuration saved in test-summarization\\checkpoint-9000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-9000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [test-summarization\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-9500\n",
      "Configuration saved in test-summarization\\checkpoint-9500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-9500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-10000\n",
      "Configuration saved in test-summarization\\checkpoint-10000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-10500\n",
      "Configuration saved in test-summarization\\checkpoint-10500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-10500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-11000\n",
      "Configuration saved in test-summarization\\checkpoint-11000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-11000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-11500\n",
      "Configuration saved in test-summarization\\checkpoint-11500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-11500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-12000\n",
      "Configuration saved in test-summarization\\checkpoint-12000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-12500\n",
      "Configuration saved in test-summarization\\checkpoint-12500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-12500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-13000\n",
      "Configuration saved in test-summarization\\checkpoint-13000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-13000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-13500\n",
      "Configuration saved in test-summarization\\checkpoint-13500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-13500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-14000\n",
      "Configuration saved in test-summarization\\checkpoint-14000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-14000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-14500\n",
      "Configuration saved in test-summarization\\checkpoint-14500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-14500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-15000\n",
      "Configuration saved in test-summarization\\checkpoint-15000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-15000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-15500\n",
      "Configuration saved in test-summarization\\checkpoint-15500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-15500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-16000\n",
      "Configuration saved in test-summarization\\checkpoint-16000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-16000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-16500\n",
      "Configuration saved in test-summarization\\checkpoint-16500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-16500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-17000\n",
      "Configuration saved in test-summarization\\checkpoint-17000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-17000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-15500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article_original, media, extractive, abstractive.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8561\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to test-summarization\\checkpoint-17500\n",
      "Configuration saved in test-summarization\\checkpoint-17500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-summarization\\checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-17500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-18000\n",
      "Configuration saved in test-summarization\\checkpoint-18000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-18000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-18500\n",
      "Configuration saved in test-summarization\\checkpoint-18500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-18500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-19000\n",
      "Configuration saved in test-summarization\\checkpoint-19000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-19000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-19500\n",
      "Configuration saved in test-summarization\\checkpoint-19500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-19500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-19500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-19500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-20000\n",
      "Configuration saved in test-summarization\\checkpoint-20000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-20000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-20500\n",
      "Configuration saved in test-summarization\\checkpoint-20500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-20500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-20500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-20500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-21000\n",
      "Configuration saved in test-summarization\\checkpoint-21000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-21000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-21000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-21000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-21500\n",
      "Configuration saved in test-summarization\\checkpoint-21500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-21500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-21500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-21500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-22000\n",
      "Configuration saved in test-summarization\\checkpoint-22000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-22000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-22500\n",
      "Configuration saved in test-summarization\\checkpoint-22500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-22500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-22500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-22500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-23000\n",
      "Configuration saved in test-summarization\\checkpoint-23000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-23000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-23000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-23000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-23500\n",
      "Configuration saved in test-summarization\\checkpoint-23500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-23500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-23500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-23500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-24000\n",
      "Configuration saved in test-summarization\\checkpoint-24000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-24000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-24500\n",
      "Configuration saved in test-summarization\\checkpoint-24500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-24500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-24500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-24500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-25000\n",
      "Configuration saved in test-summarization\\checkpoint-25000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-25000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-25500\n",
      "Configuration saved in test-summarization\\checkpoint-25500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-25500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-25500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-25500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-26000\n",
      "Configuration saved in test-summarization\\checkpoint-26000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-summarization\\checkpoint-26000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-26000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-26000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-26500\n",
      "Configuration saved in test-summarization\\checkpoint-26500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-26500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-26500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-26500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-27000\n",
      "Configuration saved in test-summarization\\checkpoint-27000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-27000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-27000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-27000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-27500\n",
      "Configuration saved in test-summarization\\checkpoint-27500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-27500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-27500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-27500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-28000\n",
      "Configuration saved in test-summarization\\checkpoint-28000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-28000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-28000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-28000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-28500\n",
      "Configuration saved in test-summarization\\checkpoint-28500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-28500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-28500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-28500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-29000\n",
      "Configuration saved in test-summarization\\checkpoint-29000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-29000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-29000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-29000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-29500\n",
      "Configuration saved in test-summarization\\checkpoint-29500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-29500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-29500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-29500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-30000\n",
      "Configuration saved in test-summarization\\checkpoint-30000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-30000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-30500\n",
      "Configuration saved in test-summarization\\checkpoint-30500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-30500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-30500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-30500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-31000\n",
      "Configuration saved in test-summarization\\checkpoint-31000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-31000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-31000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-31000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-31500\n",
      "Configuration saved in test-summarization\\checkpoint-31500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-31500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-31500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-31500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-32000\n",
      "Configuration saved in test-summarization\\checkpoint-32000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-32000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-32000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-32000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-32500\n",
      "Configuration saved in test-summarization\\checkpoint-32500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-32500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-32500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-32500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-33000\n",
      "Configuration saved in test-summarization\\checkpoint-33000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-33000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-33000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-33000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-33500\n",
      "Configuration saved in test-summarization\\checkpoint-33500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-33500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-33500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-33500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-34000\n",
      "Configuration saved in test-summarization\\checkpoint-34000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-34000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-34000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-34000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-32500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article_original, media, extractive, abstractive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8561\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to test-summarization\\checkpoint-34500\n",
      "Configuration saved in test-summarization\\checkpoint-34500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-34500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-34500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-34500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-35000\n",
      "Configuration saved in test-summarization\\checkpoint-35000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-35000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-35000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-35000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-35500\n",
      "Configuration saved in test-summarization\\checkpoint-35500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-35500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-35500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-35500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-36000\n",
      "Configuration saved in test-summarization\\checkpoint-36000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-36000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-36000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-36000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-36500\n",
      "Configuration saved in test-summarization\\checkpoint-36500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-36500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-36500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-36500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-37000\n",
      "Configuration saved in test-summarization\\checkpoint-37000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-37000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-37000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-37000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-37500\n",
      "Configuration saved in test-summarization\\checkpoint-37500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-37500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-37500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-37500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-38000\n",
      "Configuration saved in test-summarization\\checkpoint-38000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-38000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-38000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-38000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-38500\n",
      "Configuration saved in test-summarization\\checkpoint-38500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-38500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-38500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-38500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-39000\n",
      "Configuration saved in test-summarization\\checkpoint-39000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-39000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-39000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-39000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-39500\n",
      "Configuration saved in test-summarization\\checkpoint-39500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-39500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-39500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-39500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-40000\n",
      "Configuration saved in test-summarization\\checkpoint-40000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-40000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-40500\n",
      "Configuration saved in test-summarization\\checkpoint-40500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-40500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-40500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-40500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-41000\n",
      "Configuration saved in test-summarization\\checkpoint-41000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-41000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-41000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-41000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-41500\n",
      "Configuration saved in test-summarization\\checkpoint-41500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-41500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-41500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-41500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-42000\n",
      "Configuration saved in test-summarization\\checkpoint-42000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-42000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-42000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-42000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-42500\n",
      "Configuration saved in test-summarization\\checkpoint-42500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-42500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-42500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-42500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-41000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-summarization\\checkpoint-43000\n",
      "Configuration saved in test-summarization\\checkpoint-43000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-43000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-43000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-43000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-43500\n",
      "Configuration saved in test-summarization\\checkpoint-43500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-43500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-43500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-43500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-44000\n",
      "Configuration saved in test-summarization\\checkpoint-44000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-44000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-44000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-44000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-44500\n",
      "Configuration saved in test-summarization\\checkpoint-44500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-44500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-44500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-44500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-45000\n",
      "Configuration saved in test-summarization\\checkpoint-45000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-45000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-45000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-45000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-45500\n",
      "Configuration saved in test-summarization\\checkpoint-45500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-45500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-45500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-45500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-46000\n",
      "Configuration saved in test-summarization\\checkpoint-46000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-46000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-46000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-46000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-46500\n",
      "Configuration saved in test-summarization\\checkpoint-46500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-46500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-46500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-46500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-47000\n",
      "Configuration saved in test-summarization\\checkpoint-47000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-47000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-47000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-47000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-47500\n",
      "Configuration saved in test-summarization\\checkpoint-47500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-47500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-47500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-47500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-48000\n",
      "Configuration saved in test-summarization\\checkpoint-48000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-48000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-48000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-48000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-48500\n",
      "Configuration saved in test-summarization\\checkpoint-48500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-48500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-48500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-48500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-49000\n",
      "Configuration saved in test-summarization\\checkpoint-49000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-49000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-49000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-49000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-49500\n",
      "Configuration saved in test-summarization\\checkpoint-49500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-49500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-49500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-49500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-50000\n",
      "Configuration saved in test-summarization\\checkpoint-50000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-50000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-50000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-50000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-50500\n",
      "Configuration saved in test-summarization\\checkpoint-50500\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-50500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-50500\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-50500\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to test-summarization\\checkpoint-51000\n",
      "Configuration saved in test-summarization\\checkpoint-51000\\config.json\n",
      "Model weights saved in test-summarization\\checkpoint-51000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-summarization\\checkpoint-51000\\tokenizer_config.json\n",
      "Special tokens file saved in test-summarization\\checkpoint-51000\\special_tokens_map.json\n",
      "Deleting older checkpoint [test-summarization\\checkpoint-49500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article_original, media, extractive, abstractive.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8561\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51363, training_loss=1.2181168893340582, metrics={'train_runtime': 9053.6329, 'train_samples_per_second': 11.346, 'train_steps_per_second': 5.673, 'total_flos': 4.194763158144614e+16, 'train_loss': 1.2181168893340582, 'epoch': 3.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GenerationMixin.generate of BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
